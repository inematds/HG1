---
sidebar_position: 8
title: 8. IA e Aprendizado
description: RL, LLMs e imitation learning
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# ðŸ§  IA e Aprendizado

:::tip Objetivo do MÃ³dulo
Dominar tÃ©cnicas de IA para robÃ³tica humanoide: Reinforcement Learning (RL) para controle robusto, Large Language Models (LLMs) para reasoning e planejamento de alto nÃ­vel, e Imitation Learning para aprender de demonstraÃ§Ãµes humanas. Essas sÃ£o as fronteiras da pesquisa em humanoides.
:::

**DuraÃ§Ã£o estimada:** 55 minutos
**PrÃ©-requisitos:** MÃ³dulos 1-7 (Python, ROS2, ManipulaÃ§Ã£o)

---

## ðŸŽ® Reinforcement Learning para Controle

### Conceitos Fundamentais

```
Agente (RobÃ´) interage com Ambiente:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            AMBIENTE (Mundo Real)        â”‚
â”‚  Estado: s_t = [posiÃ§Ã£o, velocidade,    â”‚
â”‚                  forÃ§a, visÃ£o, ...]     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ Estado s_t            â”‚ AÃ§Ã£o a_t
           â”‚                       â”‚
           â–¼                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
â”‚         AGENTE (PolÃ­tica Ï€)              â”‚
â”‚  Input: s_t                              â”‚
â”‚  Neural Network: [Dense â†’ ReLU â†’ ...]   â”‚
â”‚  Output: a_t = Ï€(s_t)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ Recompensa r_t
           â–¼
    Objetivo: Maximizar Î£ r_t
```

**EquaÃ§Ã£o de Bellman:**
```
Q(s, a) = r + Î³ * max_a' Q(s', a')

Onde:
- Q(s, a): Valor de tomar aÃ§Ã£o 'a' no estado 's'
- r: Recompensa imediata
- Î³: Fator de desconto (0.99 tÃ­pico)
- s': PrÃ³ximo estado
```

---

## ðŸ‹ï¸ Treinando PolÃ­tica com Stable-Baselines3

### Setup

```bash
# Instalar bibliotecas de RL
pip install stable-baselines3[extra]
pip install gymnasium
pip install pybullet  # Simulador de fÃ­sica
```

### Ambiente Customizado: Humanoid Walk

```python
import gymnasium as gym
from gymnasium import spaces
import numpy as np
import pybullet as p
import pybullet_data

class HumanoidWalkEnv(gym.Env):
    """
    Ambiente de treinamento: humanoide aprendendo a andar.

    ObservaÃ§Ãµes: 44 dimensÃµes (posiÃ§Ãµes e velocidades de juntas)
    AÃ§Ãµes: 17 dimensÃµes (torques para cada junta)
    Recompensa: DistÃ¢ncia percorrida - custo de energia
    """

    def __init__(self, render=False):
        super().__init__()

        # Conectar ao PyBullet
        if render:
            p.connect(p.GUI)
        else:
            p.connect(p.DIRECT)

        p.setAdditionalSearchPath(pybullet_data.getDataPath())
        p.setGravity(0, 0, -9.81)

        # Definir espaÃ§os de observaÃ§Ã£o e aÃ§Ã£o
        # 44 observaÃ§Ãµes: 17 juntas Ã— (posiÃ§Ã£o + velocidade) + orientaÃ§Ã£o (4)
        self.observation_space = spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=(44,),
            dtype=np.float32
        )

        # 17 aÃ§Ãµes: torques para cada junta (-1 a 1, normalizado)
        self.action_space = spaces.Box(
            low=-1.0,
            high=1.0,
            shape=(17,),
            dtype=np.float32
        )

        # Estado
        self.robot_id = None
        self.initial_position = [0, 0, 1.0]  # 1 metro de altura

    def reset(self, seed=None, options=None):
        """Resetar ambiente para novo episÃ³dio."""
        super().reset(seed=seed)

        p.resetSimulation()
        p.setGravity(0, 0, -9.81)

        # Carregar chÃ£o e robÃ´
        p.loadURDF("plane.urdf")
        self.robot_id = p.loadURDF(
            "humanoid/humanoid.urdf",
            self.initial_position,
            useFixedBase=False
        )

        # Estado inicial
        observation = self._get_observation()
        info = {}

        return observation, info

    def step(self, action):
        """
        Executar aÃ§Ã£o e retornar prÃ³ximo estado.

        Args:
            action: array de torques (17 dimensÃµes)

        Returns:
            observation, reward, terminated, truncated, info
        """
        # ====================================
        # APLICAR AÃ‡Ã•ES (Torques nas juntas)
        # ====================================
        max_torque = 100.0  # Newtons-metro

        for joint_idx in range(17):
            p.setJointMotorControl2(
                self.robot_id,
                joint_idx,
                p.TORQUE_CONTROL,
                force=action[joint_idx] * max_torque
            )

        # Simular fÃ­sica
        p.stepSimulation()

        # ====================================
        # OBTER NOVO ESTADO
        # ====================================
        observation = self._get_observation()

        # ====================================
        # CALCULAR RECOMPENSA
        # ====================================
        reward = self._compute_reward(action)

        # ====================================
        # VERIFICAR TÃ‰RMINO
        # ====================================
        terminated = self._is_terminated()
        truncated = False  # EpisÃ³dio muito longo

        info = {}

        return observation, reward, terminated, truncated, info

    def _get_observation(self):
        """
        Construir vetor de observaÃ§Ã£o (44 dimensÃµes).
        """
        obs = []

        # PosiÃ§Ã£o e orientaÃ§Ã£o do torso
        pos, orn = p.getBasePositionAndOrientation(self.robot_id)
        obs.extend(orn)  # Quaternion (4 valores)

        # PosiÃ§Ã£o e velocidade de cada junta
        for joint_idx in range(17):
            joint_state = p.getJointState(self.robot_id, joint_idx)
            obs.append(joint_state[0])  # PosiÃ§Ã£o angular
            obs.append(joint_state[1])  # Velocidade angular

        return np.array(obs, dtype=np.float32)

    def _compute_reward(self, action):
        """
        FunÃ§Ã£o de recompensa para incentivar caminhada.

        Componentes:
        1. Velocidade para frente (+)
        2. Permanecer em pÃ© (+)
        3. Custo de energia (-) (torques altos)
        4. Penalidade por queda (-)
        """
        pos, orn = p.getBasePositionAndOrientation(self.robot_id)
        vel, ang_vel = p.getBaseVelocity(self.robot_id)

        # Recompensa por velocidade para frente
        forward_reward = vel[0] * 5.0  # x Ã© forward

        # Recompensa por permanecer em pÃ© (altura > 0.8m)
        height_reward = max(0, pos[2] - 0.8) * 2.0

        # Penalidade por energia (torques altos)
        energy_cost = -0.01 * np.sum(np.square(action))

        # Penalidade por queda
        if pos[2] < 0.5:  # Caiu
            fall_penalty = -10.0
        else:
            fall_penalty = 0.0

        # Recompensa total
        reward = forward_reward + height_reward + energy_cost + fall_penalty

        return reward

    def _is_terminated(self):
        """EpisÃ³dio termina se robÃ´ cair."""
        pos, _ = p.getBasePositionAndOrientation(self.robot_id)
        return pos[2] < 0.5  # Altura < 50cm = caiu

    def close(self):
        p.disconnect()
```

### Treinar PolÃ­tica com PPO

```python
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv
from stable_baselines3.common.callbacks import CheckpointCallback

def train_humanoid_walk():
    """
    Treinar humanoide para andar usando PPO.

    PPO (Proximal Policy Optimization):
    - State-of-the-art para robÃ³tica
    - EstÃ¡vel (nÃ£o diverge facilmente)
    - Sample-efficient (aprende rÃ¡pido)
    """

    # ====================================
    # CRIAR AMBIENTES PARALELOS
    # ====================================
    # 16 robÃ´s treinando simultaneamente (acelera 16x)
    def make_env():
        def _init():
            return HumanoidWalkEnv(render=False)
        return _init

    num_envs = 16
    env = SubprocVecEnv([make_env() for _ in range(num_envs)])

    # ====================================
    # CONFIGURAR ALGORITMO PPO
    # ====================================
    model = PPO(
        policy="MlpPolicy",  # Multi-Layer Perceptron (rede neural)
        env=env,
        learning_rate=3e-4,
        n_steps=2048,        # Passos por update
        batch_size=64,
        n_epochs=10,
        gamma=0.99,          # Fator de desconto
        gae_lambda=0.95,     # Generalized Advantage Estimation
        clip_range=0.2,
        ent_coef=0.01,       # Entropia (exploraÃ§Ã£o)
        verbose=1,
        tensorboard_log="./logs/humanoid_walk/"
    )

    # ====================================
    # CALLBACKS
    # ====================================
    # Salvar checkpoint a cada 100k steps
    checkpoint_callback = CheckpointCallback(
        save_freq=100000,
        save_path="./models/humanoid_walk/",
        name_prefix="ppo_humanoid"
    )

    # ====================================
    # TREINAR
    # ====================================
    total_timesteps = 10_000_000  # 10 milhÃµes de passos

    print(f"Iniciando treinamento por {total_timesteps:,} timesteps...")
    print("Isso pode levar vÃ¡rias horas dependendo do hardware.")

    model.learn(
        total_timesteps=total_timesteps,
        callback=checkpoint_callback,
        progress_bar=True
    )

    # Salvar modelo final
    model.save("humanoid_walk_final")

    print("âœ… Treinamento completo!")

    # ====================================
    # TESTAR POLÃTICA TREINADA
    # ====================================
    env_test = HumanoidWalkEnv(render=True)
    obs, _ = env_test.reset()

    for _ in range(1000):
        action, _ = model.predict(obs, deterministic=True)
        obs, reward, terminated, truncated, _ = env_test.step(action)

        if terminated or truncated:
            obs, _ = env_test.reset()

    env_test.close()

if __name__ == "__main__":
    train_humanoid_walk()
```

### Monitorar Treinamento com TensorBoard

```bash
# Visualizar progresso em tempo real
tensorboard --logdir=./logs/humanoid_walk/

# Abrir no navegador: http://localhost:6006

# MÃ©tricas importantes:
# - rollout/ep_rew_mean: Recompensa mÃ©dia (deve aumentar)
# - train/policy_loss: Loss da polÃ­tica (deve estabilizar)
# - train/value_loss: Loss do critic
```

---

## ðŸ¤– LLMs para RobÃ³tica

### IntegraÃ§Ã£o LLM + RobÃ´ (VLMs)

```python
import openai
from openai import OpenAI
import base64

class LLMRobotPlanner(Node):
    """
    Usar LLM (GPT-4, Claude) para planejamento de alto nÃ­vel.

    Exemplo:
    - Humano: "Organize a mesa"
    - LLM: DecompÃµe em subtarefas
        1. Detectar objetos na mesa
        2. Classificar (lixo vs Ãºtil)
        3. Pegar lixo e jogar fora
        4. Organizar itens restantes
    """

    def __init__(self):
        super().__init__('llm_planner')

        # Cliente OpenAI
        self.client = OpenAI(api_key="sk-...")

        # Controladores de baixo nÃ­vel
        self.navigator = NavigationClient()
        self.arm = ArmController()
        self.vision = YOLODetector()

    def plan_task(self, instruction):
        """
        Enviar instruÃ§Ã£o para LLM e obter plano.

        Args:
            instruction: "Organize a mesa" (linguagem natural)

        Returns:
            Lista de aÃ§Ãµes primitivas
        """

        # ====================================
        # PROMPT ENGINEERING
        # ====================================
        system_prompt = """
VocÃª Ã© um robÃ´ humanoide com as seguintes capacidades:

AÃ‡Ã•ES DISPONÃVEIS:
- navigate_to(location): Navegar atÃ© local
- pick_object(object_id): Pegar objeto
- place_object(location): Colocar objeto
- open_gripper(): Abrir mÃ£o
- close_gripper(): Fechar mÃ£o

LOCAIS CONHECIDOS:
- "table", "trash_bin", "shelf", "charging_station"

Dada uma instruÃ§Ã£o, decomponha em uma sequÃªncia de aÃ§Ãµes.
Retorne JSON: [{"action": "navigate_to", "args": {"location": "table"}}, ...]
"""

        user_message = f"""
InstruÃ§Ã£o: {instruction}

Contexto atual:
- PosiÃ§Ã£o do robÃ´: na charging_station
- Objetos detectados: copo (id=1), papel amassado (id=2), caneta (id=3)
- Bateria: 85%

Retorne o plano em JSON.
"""

        # ====================================
        # CHAMAR API
        # ====================================
        response = self.client.chat.completions.create(
            model="gpt-4-turbo",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message}
            ],
            temperature=0.2,  # Baixo = mais determinÃ­stico
            response_format={"type": "json_object"}
        )

        # Parse JSON
        plan = json.loads(response.choices[0].message.content)

        self.get_logger().info(f"Plano gerado pelo LLM:\n{json.dumps(plan, indent=2)}")

        return plan["actions"]

    def execute_plan(self, actions):
        """
        Executar sequÃªncia de aÃ§Ãµes.
        """
        for action in actions:
            action_name = action["action"]
            args = action.get("args", {})

            self.get_logger().info(f"Executando: {action_name}({args})")

            if action_name == "navigate_to":
                location = args["location"]
                # Mapear location â†’ coordenadas
                coords = self.location_to_coords(location)
                self.navigator.navigate_to_pose(*coords)

            elif action_name == "pick_object":
                object_id = args["object_id"]
                # Usar visÃ£o para localizar objeto
                object_pose = self.vision.get_object_pose(object_id)
                # Pegar com braÃ§o
                self.arm.pick(object_pose)

            elif action_name == "place_object":
                location = args["location"]
                place_pose = self.location_to_coords(location)
                self.arm.place(place_pose)

            elif action_name == "open_gripper":
                self.arm.open_gripper()

            elif action_name == "close_gripper":
                self.arm.close_gripper()

            else:
                self.get_logger().error(f"AÃ§Ã£o desconhecida: {action_name}")

        self.get_logger().info("âœ… Plano executado completamente!")

# Usar:
planner = LLMRobotPlanner()

instruction = "Organize a mesa: jogue o lixo fora e coloque os itens Ãºteis na prateleira"
plan = planner.plan_task(instruction)
planner.execute_plan(plan)
```

### Vision-Language Models (VLMs)

```python
def plan_with_vision(self, instruction, camera_image):
    """
    Usar VLM (GPT-4 Vision) para planejar baseado em imagem.
    """

    # Converter imagem para base64
    _, buffer = cv2.imencode('.jpg', camera_image)
    img_base64 = base64.b64encode(buffer).decode('utf-8')

    response = self.client.chat.completions.create(
        model="gpt-4-vision-preview",
        messages=[
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": instruction},
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{img_base64}"
                        }
                    }
                ]
            }
        ],
        max_tokens=500
    )

    return response.choices[0].message.content

# Exemplo:
instruction = "O que vocÃª vÃª na mesa? O que deveria fazer para organizÃ¡-la?"
camera_img = get_current_camera_image()
response = planner.plan_with_vision(instruction, camera_img)

print(response)
# "Vejo um copo, papel amassado e uma caneta. Devo jogar o papel no lixo
#  e organizar o copo e a caneta na prateleira."
```

---

## ðŸ‘¨â€ðŸ« Imitation Learning

### Aprender de DemonstraÃ§Ãµes Humanas

```python
from torch import nn
import torch

class BehaviorCloning:
    """
    Behavioral Cloning: Aprender polÃ­tica supervisionada
    de demonstraÃ§Ãµes de especialista (humano).

    Dataset: (observaÃ§Ã£o, aÃ§Ã£o) pares gravados
    Modelo: Neural network que mapeia obs â†’ aÃ§Ã£o
    """

    def __init__(self, obs_dim=44, action_dim=17):
        # Rede neural simples
        self.policy = nn.Sequential(
            nn.Linear(obs_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Linear(256, action_dim),
            nn.Tanh()  # AÃ§Ãµes entre -1 e 1
        )

        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=1e-3)
        self.criterion = nn.MSELoss()

    def collect_demonstrations(self, num_episodes=100):
        """
        Coletar demonstraÃ§Ãµes de humano (teleoperaÃ§Ã£o).
        """
        demonstrations = []

        for episode in range(num_episodes):
            obs = env.reset()
            done = False

            while not done:
                # Humano controla robÃ´ via joystick/VR
                action = get_human_action()  # Implementar teleop

                next_obs, reward, done, _ = env.step(action)

                # Salvar par (obs, aÃ§Ã£o)
                demonstrations.append({
                    'observation': obs,
                    'action': action
                })

                obs = next_obs

        return demonstrations

    def train(self, demonstrations, epochs=100):
        """
        Treinar polÃ­tica por imitaÃ§Ã£o.
        """
        # Converter para tensors
        obs = torch.FloatTensor([d['observation'] for d in demonstrations])
        actions = torch.FloatTensor([d['action'] for d in demonstrations])

        dataset = torch.utils.data.TensorDataset(obs, actions)
        loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)

        for epoch in range(epochs):
            total_loss = 0

            for batch_obs, batch_actions in loader:
                # Forward
                predicted_actions = self.policy(batch_obs)

                # Loss: diferenÃ§a entre aÃ§Ã£o prevista e aÃ§Ã£o expert
                loss = self.criterion(predicted_actions, batch_actions)

                # Backward
                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()

                total_loss += loss.item()

            print(f"Epoch {epoch}: Loss = {total_loss / len(loader):.4f}")

    def predict(self, observation):
        """Prever aÃ§Ã£o dada observaÃ§Ã£o."""
        obs_tensor = torch.FloatTensor(observation).unsqueeze(0)
        with torch.no_grad():
            action = self.policy(obs_tensor)
        return action.numpy()[0]

# Usar:
bc = BehaviorCloning()

# 1. Coletar demos
demos = bc.collect_demonstrations(num_episodes=50)

# 2. Treinar
bc.train(demos, epochs=200)

# 3. Usar polÃ­tica treinada
obs = env.reset()
action = bc.predict(obs)
```

---

## ðŸ­ Casos de Uso em ProduÃ§Ã£o

### Tesla Optimus

**Stack de IA:**
- **LocomoÃ§Ã£o:** RL (PPO) treinado em simulaÃ§Ã£o, fine-tuned no real
- **ManipulaÃ§Ã£o:** Imitation learning de demos + RL para refinamento
- **Planejamento:** LLM (modelo interno) para task decomposition
- **VisÃ£o:** Neural networks end-to-end (nÃ£o YOLO tradicional)

**Caso:** Dobrar camiseta
1. LLM decompÃµe: "pegar canto A", "dobrar ao meio", "pegar canto B", ...
2. RL controla braÃ§os para executar cada primitiva
3. VisÃ£o rastreia camiseta durante movimento
4. Force control ajusta grip para nÃ£o amassar

### Figure 01

**Abordagem hÃ­brida:**
- **LLM:** RaciocÃ­nio de alto nÃ­vel (linguagem natural)
- **Classical planning:** SequÃªncias de aÃ§Ãµes (PDDL)
- **RL:** Controle de baixo nÃ­vel (juntas)

---

## âœ… Checklist de DomÃ­nio

Antes de avanÃ§ar, certifique-se de que vocÃª consegue:

- [ ] Entender conceitos de RL (MDP, polÃ­tica, recompensa)
- [ ] Criar ambiente Gym customizado
- [ ] Treinar polÃ­tica com Stable-Baselines3 (PPO)
- [ ] Integrar LLM para task planning
- [ ] Usar Vision-Language Models (VLMs)
- [ ] Implementar Behavioral Cloning
- [ ] Coletar demonstraÃ§Ãµes de humanos
- [ ] Combinar RL + Imitation Learning
- [ ] Monitorar treinamento com TensorBoard
- [ ] Transferir polÃ­tica de simulaÃ§Ã£o â†’ real (sim-to-real)

---

## ðŸ”— PrÃ³ximos Passos

:::tip PrÃ³ximo MÃ³dulo
**[ðŸ”— IntegraÃ§Ã£o de Sistemas â†’](./integracao-sistemas)**

Integrar todos os mÃ³dulos (navegaÃ§Ã£o, visÃ£o, manipulaÃ§Ã£o, IA) em um sistema coeso. Deployment, otimizaÃ§Ã£o e debug de sistemas completos.
:::

**Recursos adicionais:**
- [Stable-Baselines3 Docs](https://stable-baselines3.readthedocs.io/)
- [OpenAI Gymnasium](https://gymnasium.farama.org/)
- [Spinning Up in RL](https://spinningup.openai.com/)
- [LangChain for Robotics](https://python.langchain.com/docs/use_cases/robotics)
