---
sidebar_position: 6
title: 6. Sensores e PercepÃ§Ã£o
description: LIDAR, cÃ¢meras e IMU
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# ğŸ“¡ Sensores e PercepÃ§Ã£o

:::tip Objetivo
Entender como o Unitree G1 "enxerga" e percebe o mundo atravÃ©s de seus sensores: LIDAR 3D, cÃ¢meras RGB-D, IMU, encoders e sensores de forÃ§a.
:::

---

## ğŸ” VisÃ£o Geral dos Sensores

O G1 possui **mÃºltiplos sistemas de sensores** trabalhando em conjunto para percepÃ§Ã£o completa do ambiente.

### Hierarquia de Sensores

```
STACK DE PERCEPÃ‡ÃƒO DO G1:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1ï¸âƒ£ VISÃƒO (Exteroceptivo)
   â”œâ”€ LIDAR 3D (Ouster OS1-64) â†’ Mapeamento ambiente
   â”œâ”€ RGB-D Camera (Intel RealSense D435) â†’ DetecÃ§Ã£o objetos
   â””â”€ Stereo Cameras (2x) â†’ Profundidade e tracking

2ï¸âƒ£ PROPRIOCEPÃ‡ÃƒO (Interoceptivo)
   â”œâ”€ IMU (9-axis) â†’ OrientaÃ§Ã£o e aceleraÃ§Ã£o
   â”œâ”€ Joint Encoders (29x) â†’ PosiÃ§Ã£o das juntas
   â””â”€ Force/Torque Sensors (pÃ©s e mÃ£os) â†’ Contato

3ï¸âƒ£ OUTROS
   â”œâ”€ Microphones (4x) â†’ Ãudio espacial
   â””â”€ Temperature Sensors â†’ Monitorar motores
```

---

## ğŸ“¸ Sistema de CÃ¢meras

### CÃ¢mera RGB-D (CabeÃ§a)

<Tabs>
<TabItem value="specs" label="EspecificaÃ§Ãµes">

**Intel RealSense D435:**
```
SPECS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â€¢ ResoluÃ§Ã£o RGB: 1920x1080 @ 30fps
â€¢ ResoluÃ§Ã£o Depth: 1280x720 @ 30fps
â€¢ Range: 0.3m - 10m
â€¢ FOV (H x V): 87Â° x 58Â°
â€¢ PrecisÃ£o depth: Â±2% atÃ© 4m
â€¢ Framerate: atÃ© 90fps (baixa resoluÃ§Ã£o)
```

**Capacidades:**
- âœ… DetecÃ§Ã£o de objetos (YOLO, Mask R-CNN)
- âœ… SegmentaÃ§Ã£o semÃ¢ntica
- âœ… EstimaÃ§Ã£o de pose humana
- âœ… Reconhecimento facial
- âœ… OCR (ler texto)

</TabItem>

<TabItem value="access" label="Acessar Dados">

### Capturar Imagens

```python
from unitree_sdk import G1Vision

vision = G1Vision()

# Capturar frame RGB
rgb_frame = vision.get_rgb_frame()
print(f"Shape: {rgb_frame.shape}")  # (1080, 1920, 3)

# Capturar mapa de profundidade
depth_frame = vision.get_depth_frame()
print(f"Shape: {depth_frame.shape}")  # (720, 1280)

# Capturar ambos sincronizados
rgb, depth = vision.get_rgbd_frame()

# Salvar imagem
vision.save_image(rgb_frame, "/tmp/snapshot.jpg")
```

### Stream de VÃ­deo

```python
import cv2

# Stream contÃ­nuo
for frame in vision.stream_rgb(fps=30):
    # Processar frame
    cv2.imshow("G1 Vision", frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

vision.stop_stream()
```

</TabItem>

<TabItem value="detection" label="DetecÃ§Ã£o de Objetos">

### Detectar Objetos (YOLO)

```python
# Detectar objetos comuns
detections = vision.detect_objects(
    model="yolov8",
    confidence=0.6,      # MÃ­nimo 60% confianÃ§a
    classes=["person", "cup", "bottle", "laptop"]
)

for obj in detections:
    print(f"Objeto: {obj.class_name}")
    print(f"ConfianÃ§a: {obj.confidence:.2%}")
    print(f"BBox: {obj.bbox}")  # (x, y, w, h)
    print(f"PosiÃ§Ã£o 3D: {obj.position_3d}")  # (x, y, z) em metros
    print(f"DistÃ¢ncia: {obj.distance:.2f}m")

# Filtrar por classe
cups = vision.filter_objects(detections, class_name="cup")
print(f"Encontrados {len(cups)} copos")
```

### SegmentaÃ§Ã£o de InstÃ¢ncia

```python
# Segmentar objetos pixel-a-pixel
segments = vision.segment_instances(
    model="mask_rcnn",
    min_area=100  # pixels mÃ­nimos
)

for seg in segments:
    print(f"Objeto: {seg.class_name}")
    print(f"MÃ¡scara shape: {seg.mask.shape}")
    print(f"Ãrea: {seg.area} pixels")

    # Extrair submÃ¡scara
    object_only = rgb_frame * seg.mask[:, :, None]
    vision.save_image(object_only, f"/tmp/{seg.class_name}.jpg")
```

</TabItem>
</Tabs>

### CÃ¢meras EstÃ©reo (Laterais)

**2x CÃ¢meras monocromÃ¡ticas:**
```python
# Acessar cÃ¢meras esquerda/direita
left_img = vision.get_stereo_left()
right_img = vision.get_stereo_right()

# Calcular disparidade (profundidade)
disparity = vision.compute_stereo_disparity(left_img, right_img)

# Converter para nuvem de pontos
point_cloud = vision.disparity_to_pointcloud(disparity)
print(f"Pontos: {len(point_cloud)}")
```

---

## ğŸŒ LIDAR 3D

### Ouster OS1-64

<Tabs>
<TabItem value="specs" label="EspecificaÃ§Ãµes">

```
LIDAR SPECS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â€¢ Canais: 64 (lasers verticais)
â€¢ Range: 0.5m - 120m
â€¢ PrecisÃ£o: Â±3cm atÃ© 50m
â€¢ FOV: 360Â° H x 45Â° V
â€¢ FrequÃªncia: 10 ou 20 Hz
â€¢ Pontos/segundo: 1.3 milhÃµes
â€¢ ResistÃªncia: IP68 (Ã¡gua/poeira)
```

**Montagem:** Centro do peito do G1

</TabItem>

<TabItem value="pointcloud" label="Nuvem de Pontos">

### Acessar Point Cloud

```python
from unitree_sdk import G1LIDAR

lidar = G1LIDAR()

# Capturar scan Ãºnico
scan = lidar.get_scan()
print(f"Pontos: {len(scan.points)}")  # ~1.3M pontos

# Acessar pontos (numpy array)
points = scan.points  # Shape: (N, 3) - (x, y, z)
intensities = scan.intensities  # Refletividade

# Filtrar por distÃ¢ncia
close_points = points[points[:, 2] < 2.0]  # Apenas < 2m
print(f"Pontos prÃ³ximos: {len(close_points)}")

# Salvar para visualizaÃ§Ã£o
lidar.save_pcd(scan, "/tmp/scan.pcd")  # Formato PCD
```

### Visualizar em Tempo Real

```bash
# Usando ROS2 + RViz
ros2 launch unitree_lidar visualize.launch.py

# Ou Python
import open3d as o3d

pcd = o3d.io.read_point_cloud("/tmp/scan.pcd")
o3d.visualization.draw_geometries([pcd])
```

</TabItem>

<TabItem value="processing" label="Processamento">

### DetecÃ§Ã£o de ObstÃ¡culos

```python
# Detectar obstÃ¡culos na frente
obstacles = lidar.detect_obstacles(
    min_distance=0.5,     # MÃ­nimo 50cm
    max_distance=5.0,     # MÃ¡ximo 5m
    height_range=(0.1, 2.0),  # Entre 10cm e 2m altura
    cluster_tolerance=0.1  # Agrupar pontos < 10cm
)

for obs in obstacles:
    print(f"ObstÃ¡culo em: {obs.centroid}")
    print(f"Tamanho: {obs.dimensions}")  # (largura, altura, profundidade)
    print(f"DistÃ¢ncia: {obs.distance:.2f}m")

# AÃ§Ã£o baseada em obstÃ¡culos
if any(obs.distance < 0.8 for obs in obstacles):
    print("âš ï¸ OBSTÃCULO MUITO PERTO!")
    g1_controller.emergency_stop()
```

### SegmentaÃ§Ã£o de ChÃ£o

```python
# Identificar chÃ£o vs objetos
ground, non_ground = lidar.segment_ground(
    angle_threshold=10,  # graus
    distance_threshold=0.05  # metros
)

print(f"Pontos do chÃ£o: {len(ground)}")
print(f"Pontos de objetos: {len(non_ground)}")

# Detectar apenas objetos acima do chÃ£o
objects = lidar.cluster_objects(non_ground, eps=0.15)
```

</TabItem>
</Tabs>

---

## ğŸ§­ IMU (Unidade de MediÃ§Ã£o Inercial)

### 9-Axis IMU

<Tabs>
<TabItem value="data" label="Dados do IMU">

```python
from unitree_sdk import G1IMU

imu = G1IMU()

# Ler dados do IMU
data = imu.get_data()

# ORIENTAÃ‡ÃƒO (quaternion)
print(f"Quaternion: {data.orientation}")

# Converter para Euler angles
roll, pitch, yaw = imu.quaternion_to_euler(data.orientation)
print(f"Roll: {roll:.2f}Â°")
print(f"Pitch: {pitch:.2f}Â°")
print(f"Yaw: {yaw:.2f}Â°")

# ACELERAÃ‡ÃƒO LINEAR (m/sÂ²)
print(f"Accel: {data.linear_acceleration}")

# VELOCIDADE ANGULAR (rad/s)
print(f"Gyro: {data.angular_velocity}")
```

**FrequÃªncia:** 200 Hz (5ms por amostra)

</TabItem>

<TabItem value="applications" label="AplicaÃ§Ãµes">

### DetecÃ§Ã£o de Queda

```python
# Monitorar inclinaÃ§Ã£o para prevenir quedas
def monitor_balance():
    data = imu.get_data()
    roll, pitch, _ = imu.quaternion_to_euler(data.orientation)

    # CRÃTICO: Mais de 30Â° inclinado
    if abs(roll) > 30 or abs(pitch) > 30:
        print("ğŸš¨ RISCO DE QUEDA IMINENTE!")
        g1.emergency_balance_recovery()
        return "critical"

    # AVISO: Entre 15-30Â°
    elif abs(roll) > 15 or abs(pitch) > 15:
        print("âš ï¸ InclinaÃ§Ã£o alta")
        g1.reduce_speed(factor=0.5)
        return "warning"

    return "stable"

# Loop de monitoramento
import time
while True:
    status = monitor_balance()
    time.sleep(0.05)  # 20 Hz
```

### EstimaÃ§Ã£o de Velocidade

```python
# Integrar aceleraÃ§Ã£o para estimar velocidade
velocity = imu.estimate_velocity(
    duration=1.0,  # Integrar por 1 segundo
    filter="kalman"  # Filtro de Kalman para reduzir drift
)

print(f"Velocidade estimada: {velocity} m/s")
```

</TabItem>

<TabItem value="calibration" label="CalibraÃ§Ã£o">

### Calibrar IMU

```python
# CalibraÃ§Ã£o de fÃ¡brica
imu.reset_to_factory_calibration()

# CalibraÃ§Ã£o manual (G1 deve estar PARADO e NIVELADO)
print("Posicione G1 em superfÃ­cie plana e pressione Enter...")
input()

imu.calibrate(
    duration=10,  # Coletar dados por 10s
    auto_save=True
)

# Verificar qualidade
quality = imu.get_calibration_quality()
print(f"Qualidade: {quality}")  # 0-100%

if quality < 80:
    print("âš ï¸ CalibraÃ§Ã£o ruim! Repetir.")
```

</TabItem>
</Tabs>

---

## ğŸ¦¿ Encoders das Juntas

### 29 Encoders de PosiÃ§Ã£o

```python
from unitree_sdk import G1Joints

joints = G1Joints()

# Ler posiÃ§Ã£o de todas as juntas
positions = joints.get_positions()  # Dict com 29 juntas

print(f"Joelho esquerdo: {positions['left_knee']:.2f}Â°")
print(f"Ombro direito: {positions['right_shoulder_pitch']:.2f}Â°")

# Ler velocidades
velocities = joints.get_velocities()

# Ler torques (estimados)
torques = joints.get_torques()

# Monitorar junta especÃ­fica
left_ankle = joints.monitor_joint(
    name="left_ankle",
    rate=100  # Hz
)

for sample in left_ankle.stream(duration=5):
    print(f"Ã‚ngulo: {sample.position:.2f}Â° | Vel: {sample.velocity:.2f}Â°/s")
```

### DetecÃ§Ã£o de Anomalias

```python
# Detectar juntas com comportamento anormal
anomalies = joints.detect_anomalies(
    position_threshold=5,   # Â±5Â° do esperado
    velocity_threshold=50,  # Â±50Â°/s do esperado
    torque_threshold=10     # Â±10Nm do esperado
)

if anomalies:
    print("âš ï¸ Juntas anormais detectadas:")
    for joint, issue in anomalies.items():
        print(f"  {joint}: {issue}")
```

---

## ğŸ‘£ Sensores de ForÃ§a nos PÃ©s

### Force/Torque Sensors (4x)

```python
from unitree_sdk import G1ForceSensors

force = G1ForceSensors()

# Ler forÃ§a em cada pÃ©
left_forces = force.get_foot_forces("left")
right_forces = force.get_foot_forces("right")

print(f"ForÃ§a pÃ© esquerdo: {left_forces.total:.2f}N")
print(f"ForÃ§a pÃ© direito: {right_forces.total:.2f}N")

# Verificar distribuiÃ§Ã£o de peso
total_weight = left_forces.total + right_forces.total
left_percentage = (left_forces.total / total_weight) * 100
right_percentage = (right_forces.total / total_weight) * 100

print(f"DistribuiÃ§Ã£o: {left_percentage:.1f}% L | {right_percentage:.1f}% R")

# Detectar quando pÃ© toca o chÃ£o
if force.is_foot_in_contact("left"):
    print("âœ… PÃ© esquerdo no chÃ£o")
```

### Centro de PressÃ£o (CoP)

```python
# Calcular centro de pressÃ£o (importante para equilÃ­brio)
cop_left = force.get_center_of_pressure("left")
cop_right = force.get_center_of_pressure("right")

print(f"CoP esquerdo: {cop_left}")  # (x, y) em mm
print(f"CoP direito: {cop_right}")

# Detectar instabilidade
if cop_left[0] > 50:  # > 5cm fora do centro
    print("âš ï¸ CoP esquerdo muito deslocado!")
```

---

## ğŸ”Š Microfones (Array)

### 4x Microfones para Ãudio Espacial

```python
from unitree_sdk import G1Audio

audio = G1Audio()

# Gravar Ã¡udio
audio.start_recording(duration=5, filename="/tmp/audio.wav")
time.sleep(5)

# Detectar direÃ§Ã£o de som
sound_events = audio.detect_sound_sources()

for event in sound_events:
    print(f"Som detectado: {event.type}")
    print(f"DireÃ§Ã£o: {event.azimuth:.1f}Â° | ElevaÃ§Ã£o: {event.elevation:.1f}Â°")
    print(f"Intensidade: {event.intensity} dB")

# Localizar pessoa falando
speaker_direction = audio.localize_speech()
if speaker_direction:
    print(f"Pessoa falando em: {speaker_direction}Â°")
    g1.turn_head_to_angle(speaker_direction)
```

---

## ğŸ”¥ Sensores de Temperatura

```python
from unitree_sdk import G1Temperature

temp = G1Temperature()

# Ler temperaturas dos motores
motor_temps = temp.get_motor_temperatures()

for motor, temperature in motor_temps.items():
    if temperature > 70:
        print(f"ğŸ”¥ {motor}: {temperature}Â°C - QUENTE!")
    elif temperature > 60:
        print(f"âš ï¸ {motor}: {temperature}Â°C - Aviso")

# Temperatura mÃ©dia
avg_temp = temp.get_average_motor_temp()
print(f"Temperatura mÃ©dia: {avg_temp:.1f}Â°C")

# AÃ§Ã£o se muito quente
if avg_temp > 65:
    print("ğŸ›‘ Sistema muito quente! Reduzindo carga.")
    g1.reduce_max_speed(factor=0.6)
    g1.enable_cooling_mode()
```

---

## ğŸ§ª FusÃ£o de Sensores (Sensor Fusion)

### Combinar MÃºltiplos Sensores

```python
from unitree_sdk import G1SensorFusion

fusion = G1SensorFusion()

# SLAM (Simultaneous Localization and Mapping)
# Combina: LIDAR + IMU + Encoders + CÃ¢meras
slam = fusion.run_slam(
    mode="online",
    map_resolution=0.05,  # 5cm
    publish_tf=True
)

# Obter pose estimada (localizaÃ§Ã£o)
pose = slam.get_current_pose()
print(f"PosiÃ§Ã£o: x={pose.x:.2f}m, y={pose.y:.2f}m, Î¸={pose.theta:.2f}Â°")
print(f"ConfianÃ§a: {pose.confidence:.2%}")

# DetecÃ§Ã£o de objetos 3D (LIDAR + CÃ¢mera)
objects_3d = fusion.detect_objects_3d(
    use_lidar=True,
    use_camera=True,
    fusion_method="early"  # ou "late"
)

for obj in objects_3d:
    print(f"{obj.name}: {obj.position_3d} (confianÃ§a: {obj.confidence:.2%})")
```

---

## âœ… Checklist de PrÃ¡tica

- [ ] Capturou imagens RGB e depth da cÃ¢mera
- [ ] Detectou objetos usando visÃ£o computacional (YOLO)
- [ ] Visualizou nuvem de pontos do LIDAR
- [ ] Detectou obstÃ¡culos com LIDAR durante caminhada
- [ ] Leu dados do IMU e interpretou orientaÃ§Ã£o
- [ ] Monitorou posiÃ§Ã£o de juntas especÃ­ficas
- [ ] Verificou sensores de forÃ§a nos pÃ©s
- [ ] Testou detecÃ§Ã£o de direÃ§Ã£o de som
- [ ] Verificou temperaturas dos motores sob carga

---

## ğŸ› Troubleshooting Comum

| Problema | Causa | SoluÃ§Ã£o |
|----------|-------|---------|
| CÃ¢mera retorna frames pretos | NÃ£o inicializada | `vision.initialize()` antes de usar |
| LIDAR sem dados | ConexÃ£o USB | Verificar `lsusb`, reconectar |
| IMU com drift | NÃ£o calibrado | `imu.calibrate()` em superfÃ­cie plana |
| DetecÃ§Ã£o de objetos lenta | Modelo pesado | Usar `yolov8n` (nano) ao invÃ©s de `yolov8x` |
| Point cloud muito denso | Alta resoluÃ§Ã£o | Usar `lidar.downsample(voxel_size=0.05)` |
| Encoders imprecisos | CalibraÃ§Ã£o perdida | `joints.recalibrate_all()` |

---

## ğŸ”— PrÃ³ximos Passos

:::tip PrÃ³ximo MÃ³dulo
**[ğŸ›¡ï¸ SeguranÃ§a e Boas PrÃ¡ticas â†’](./seguranca-praticas)**

Aprenda a operar G1 de forma segura para vocÃª e para o robÃ´.
:::

---

**â±ï¸ Tempo:** 60-90 min | **ğŸ® Hands-on:** Essencial | **ğŸ’» Software:** ROS2, OpenCV, Open3D
