---
sidebar_position: 3
title: 3. Comandos de Voz
description: Controle por linguagem natural
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# ğŸ¤ Comandos de Voz

:::tip Objetivo
Dominar o sistema de comandos de voz do MindOn para controlar o G1 atravÃ©s de linguagem natural, configurar wake words, e criar interaÃ§Ãµes conversacionais complexas.
:::

---

## ğŸ¯ Fundamentos do Sistema de Voz

O MindOn utiliza um **pipeline de NLU (Natural Language Understanding)** de Ãºltima geraÃ§Ã£o para processar comandos de voz em tempo real.

### Arquitetura de Processamento

```
PIPELINE DE VOZ MINDON:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
1. Wake Word Detection (on-device)
   â””â”€> "Hey G1" ou customizado
   â””â”€> LatÃªncia: <200ms

2. Speech-to-Text (Whisper Large)
   â””â”€> Cloud: 95% accuracy
   â””â”€> Edge: 88% accuracy (Whisper-small)

3. Intent Recognition (LLM)
   â””â”€> Llama 3.1 70B (cloud) ou 8B (device)
   â””â”€> Contexto conversacional

4. Task Planning (MindOn Core)
   â””â”€> DecomposiÃ§Ã£o de tarefas
   â””â”€> ValidaÃ§Ã£o de seguranÃ§a

5. Execution + Feedback
   â””â”€> Controle motor + Skills
   â””â”€> Resposta verbal (TTS)
```

### DiferenÃ§a: Comandos Simples vs Complexos

```
COMANDO SIMPLES (direto):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"Ande para frente"
    â†“
Intent: move_forward
Action: cmd_vel linear_x=0.5
LatÃªncia: ~500ms


COMANDO COMPLEXO (multi-step):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"Traga minha mochila da cozinha"
    â†“
Plano MindOn:
    1. navigate_to_room("kitchen")
    2. find_object("backpack")
    3. pick_object("backpack")
    4. navigate_to_person()
    5. handover_object("backpack")
LatÃªncia: 30-120s (execuÃ§Ã£o completa)
```

---

## ğŸ™ï¸ Wake Word Configuration

### Wake Words PadrÃ£o

O MindOn vem com 3 wake words prÃ©-configuradas:

<Tabs>
<TabItem value="default" label="PadrÃ£o">

### "Hey G1" (Recomendado)

**ConfiguraÃ§Ã£o:**

```python
from mindon_sdk import VoiceInterface

voice = VoiceInterface()

# Ativar wake word padrÃ£o
voice.set_wake_word(
    phrase="hey g1",
    sensitivity=0.7,      # 0-1, quanto maior mais sensÃ­vel
    language="pt-BR"
)

# Testar
voice.test_wake_word()  # Fale "Hey G1" para validar
```

**CaracterÃ­sticas:**
- âœ… Baixa taxa de falsos positivos (<2%)
- âœ… Funciona com sotaques variados
- âœ… On-device (sem latÃªncia)
- âš ï¸ Pode confundir com "Hey Siri" em ambiente ruidoso

</TabItem>

<TabItem value="custom" label="Customizado">

### Wake Word Personalizado

**Criar sua prÃ³pria wake word:**

```python
# Treinar wake word customizada (requer 10 amostras)
voice.create_custom_wake_word(
    phrase="robÃ´",          # 2-3 sÃ­labas recomendado
    num_samples=10,         # Falar 10 vezes
    add_noise_samples=True  # Treinar com ruÃ­do
)

# G1 pedirÃ¡ para vocÃª repetir a palavra 10 vezes
# "Por favor, diga 'robÃ´': (1/10)"

# ApÃ³s treino
voice.set_wake_word(phrase="robÃ´", model="custom")
```

**Dicas para wake word efetiva:**
- âœ… 2-3 sÃ­labas (ex: "robÃ´", "assistente")
- âœ… Fonemas distintos
- âŒ Evitar palavras comuns ("ok", "sim")
- âŒ Evitar nomes de pessoas

</TabItem>

<TabItem value="multi" label="Multi-Wake">

### MÃºltiplas Wake Words

**Contexto-aware:**

```python
# Wake word muda baseado em contexto
voice.set_contextual_wake_words({
    "default": "hey g1",
    "security": "comando seguro",    # Modo seguranÃ§a
    "quiet": "psiu",                 # Modo silencioso
    "emergency": "emergÃªncia"        # Para emergÃªncias
})

# Mudar contexto
voice.set_wake_context("quiet")

# Agora G1 responde apenas a "psiu" (sussurrado)
```

</TabItem>
</Tabs>

### Sensibilidade e Falsos Positivos

```python
# Ajustar sensibilidade
voice.configure_detection(
    sensitivity=0.8,           # 0-1
    false_positive_threshold=0.05,  # MÃ¡x 5% FP
    noise_suppression=True,    # Filtrar ruÃ­do ambiente
    min_confidence=0.85        # ConfianÃ§a mÃ­nima
)

# Zonas de silÃªncio (nÃ£o escutar)
voice.set_quiet_zones([
    ("22:00", "07:00"),       # Noite
    ("bathroom", "always")    # Nunca no banheiro
])
```

---

## ğŸ—£ï¸ Comandos de Voz - Categorias

### NavegaÃ§Ã£o e LocomoÃ§Ã£o

<Tabs>
<TabItem value="basic-nav" label="BÃ¡sico">

**Comandos diretos:**

```python
# Exemplos de comandos reconhecidos
commands = [
    "Ande para frente",
    "Vire Ã  esquerda",
    "Pare",
    "Volte para trÃ¡s",
    "Siga-me",
    "VÃ¡ mais rÃ¡pido",
    "Caminhe devagar"
]

# ImplementaÃ§Ã£o
@voice.on_command(intent="navigate")
def handle_navigation(command):
    if "frente" in command:
        robot.walk(direction="forward", speed=0.5)
    elif "esquerda" in command:
        robot.turn(angle=-90)
    elif "pare" in command:
        robot.stop()
```

</TabItem>

<TabItem value="semantic-nav" label="SemÃ¢ntico">

**NavegaÃ§Ã£o por locais:**

```python
# Comandos semÃ¢nticos
commands = [
    "VÃ¡ para a cozinha",
    "Volte para a sala",
    "Me encontre no quarto",
    "VÃ¡ atÃ© a porta da frente",
    "Suba as escadas",
]

# MindOn mapeia automaticamente
voice.map_semantic_locations({
    "cozinha": (5.0, 2.0),
    "sala": (0.0, 0.0),
    "quarto": (8.0, 4.0),
    "porta da frente": (2.0, -3.0)
})

# Agora "VÃ¡ para cozinha" funciona automaticamente!
```

</TabItem>

<TabItem value="contextual-nav" label="Contextual">

**NavegaÃ§Ã£o contextual:**

```python
# ReferÃªncias relativas
"VÃ¡ atÃ© aquela porta"        # Usa visÃ£o para identificar
"Me siga"                    # Tracking de pessoa
"Fique a 2 metros de mim"   # DistÃ¢ncia especÃ­fica
"DÃª uma volta pela casa"    # ExploraÃ§Ã£o autÃ´noma

# ImplementaÃ§Ã£o com contexto
@voice.on_command(context_required=True)
def contextual_nav(command, context):
    if "aquela" in command:
        # Usar last_pointed_object do contexto
        target = context.get_pointed_object()
        robot.navigate_to(target.position)
```

</TabItem>
</Tabs>

### ManipulaÃ§Ã£o e InteraÃ§Ã£o

```python
# Comandos de manipulaÃ§Ã£o
manipulation_commands = {
    # Pegar objetos
    "Pegue o copo": {
        "intent": "pick",
        "object": "cup",
        "parameters": {"force": "gentle"}
    },

    # Entregar
    "Traga-me a garrafa": {
        "intent": "fetch_and_deliver",
        "object": "bottle",
        "recipient": "speaker"
    },

    # Colocar
    "Coloque isso na mesa": {
        "intent": "place",
        "object": "held_object",
        "location": "table"
    },

    # Organizar
    "Organize os livros": {
        "intent": "organize",
        "object_class": "books",
        "action": "stack"
    },

    # Abrir/Fechar
    "Abra a porta": {
        "intent": "manipulate_door",
        "action": "open"
    },
}

# Registrar handlers
@voice.on_intent("pick")
def handle_pick(object_name, parameters):
    obj = vision.find_object(object_name)
    if obj:
        manipulator.pick_object(obj, **parameters)
        voice.speak(f"Peguei o {object_name}")
    else:
        voice.speak(f"NÃ£o encontrei {object_name}")
```

### Perguntas e Consultas

```python
# Comandos informativos
query_commands = [
    "Que horas sÃ£o?",
    "Qual a temperatura aqui?",
    "CadÃª meu celular?",
    "Quanto tempo atÃ© minha reuniÃ£o?",
    "O que vocÃª estÃ¡ vendo?",
    "HÃ¡ alguÃ©m na porta?",
]

# ImplementaÃ§Ã£o
@voice.on_intent("query")
def handle_query(question):
    if "horas" in question:
        time = datetime.now().strftime("%H:%M")
        voice.speak(f"SÃ£o {time}")

    elif "temperatura" in question:
        temp = sensors.get_temperature()
        voice.speak(f"A temperatura Ã© {temp}Â°C")

    elif "cadÃª" in question or "onde" in question:
        # Usar memÃ³ria episÃ³dica
        object_name = extract_object_from_query(question)
        location = memory.recall_object_location(object_name)
        voice.speak(f"Ãšltima vez que vi, estava em {location}")
```

---

## ğŸŒ Suporte Multi-idioma

### Idiomas DisponÃ­veis

```
TIER 1 (Suporte completo - 95%+ accuracy):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â€¢ ğŸ‡ºğŸ‡¸ English (US)
â€¢ ğŸ‡§ğŸ‡· PortuguÃªs (BR)
â€¢ ğŸ‡¨ğŸ‡³ ä¸­æ–‡ (Mandarin)
â€¢ ğŸ‡ªğŸ‡¸ EspaÃ±ol (ES/LATAM)

TIER 2 (Suporte bom - 88%+ accuracy):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â€¢ ğŸ‡«ğŸ‡· FranÃ§ais
â€¢ ğŸ‡©ğŸ‡ª Deutsch
â€¢ ğŸ‡¯ğŸ‡µ æ—¥æœ¬èª
â€¢ ğŸ‡°ğŸ‡· í•œêµ­ì–´

TIER 3 (Experimental - 75%+ accuracy):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â€¢ ğŸ‡®ğŸ‡¹ Italiano
â€¢ ğŸ‡·ğŸ‡º Ğ ÑƒÑÑĞºĞ¸Ğ¹
```

### Configurar Idioma

<Tabs>
<TabItem value="single" label="Ãšnico">

**Idioma Ãºnico:**

```python
# Setar idioma principal
voice.set_language("pt-BR")

# Configurar variante regional
voice.set_language_config(
    language="pt-BR",
    dialect="brasileiro",    # vs "portugal"
    formality="informal"     # vs "formal"
)
```

</TabItem>

<TabItem value="multi" label="Multi-lÃ­ngua">

**Code-switching automÃ¡tico:**

```python
# Habilitar detecÃ§Ã£o automÃ¡tica de idioma
voice.enable_multilingual_mode(
    languages=["pt-BR", "en-US", "es-ES"],
    auto_switch=True
)

# Agora G1 entende mistura:
# "Hey G1, bring me aquela garrafa"
# â†’ Detecta inglÃªs + portuguÃªs e executa corretamente
```

</TabItem>

<TabItem value="translate" label="TraduÃ§Ã£o">

**TraduÃ§Ã£o em tempo real:**

```python
# G1 como intÃ©rprete
voice.enable_translation_mode(
    source_language="en-US",
    target_language="pt-BR"
)

# Pessoa fala inglÃªs â†’ G1 traduz para portuguÃªs
# G1 ouve portuguÃªs â†’ responde em inglÃªs
```

</TabItem>
</Tabs>

---

## ğŸ§  Contexto e MemÃ³ria Conversacional

### Manter Contexto

```python
# ConversaÃ§Ã£o com contexto
conversation = [
    ("UsuÃ¡rio", "VÃ¡ para a cozinha"),
    ("G1", "Indo para cozinha..."),

    ("UsuÃ¡rio", "Pegue o copo azul"),
    # G1 entende que copo estÃ¡ NA COZINHA (contexto)

    ("G1", "Peguei o copo"),

    ("UsuÃ¡rio", "Traga para mim"),
    # G1 entende "para mim" = usuÃ¡rio que deu comando
]

# Implementar tracking de contexto
context = voice.get_conversation_context()
print(context)
# {
#   "current_location": "kitchen",
#   "held_object": "blue_cup",
#   "last_speaker": "user_1",
#   "conversation_history": [...]
# }
```

### ReferÃªncias AnafÃ³ricas

```python
# G1 entende referÃªncias
examples = [
    "Pegue o copo vermelho. Agora lave ELE",
    # "ELE" = copo vermelho

    "VÃ¡ para sala. Depois vÃ¡ para cozinha. Agora volte para LÃ",
    # "LÃ" = sala

    "Mostre-me ISSO",
    # "ISSO" = Ãºltimo objeto mostrado/apontado
]

# Configurar resoluÃ§Ã£o anafÃ³rica
voice.enable_anaphora_resolution(
    max_history=10,          # Lembrar 10 Ãºltimos turnos
    confidence_threshold=0.8
)
```

---

## âš™ï¸ CorreÃ§Ã£o de Erros e ClarificaÃ§Ã£o

### EstratÃ©gias de CorreÃ§Ã£o

<Tabs>
<TabItem value="clarify" label="ClarificaÃ§Ã£o">

**Pedir confirmaÃ§Ã£o quando incerto:**

```python
# Configurar threshold de confianÃ§a
voice.set_clarification_policy(
    confidence_threshold=0.7,  # Pedir confirmaÃ§Ã£o se < 70%
    clarification_style="explicit"  # ou "implicit"
)

# Exemplo de interaÃ§Ã£o
# UsuÃ¡rio: "Pegue o copo" (mÃºltiplos copos visÃ­veis)
# G1: "HÃ¡ 3 copos. Qual deles? Vermelho, azul ou verde?"
# UsuÃ¡rio: "O verde"
# G1: "Ok, pegando copo verde"
```

</TabItem>

<TabItem value="retry" label="Retry">

**Retry com reformulaÃ§Ã£o:**

```python
# Configurar retry automÃ¡tico
voice.configure_retry(
    max_retries=2,
    retry_strategy="rephrase"  # ou "repeat"
)

# Exemplo
# UsuÃ¡rio: "Pegue o negÃ³cio ali"  [vago]
# G1: "NÃ£o entendi. VocÃª quer que eu pegue qual objeto?"
# UsuÃ¡rio: "O controle remoto"
# G1: "Entendido, pegando controle remoto"
```

</TabItem>

<TabItem value="correction" label="CorreÃ§Ã£o">

**Permitir correÃ§Ã£o do usuÃ¡rio:**

```python
# UsuÃ¡rio pode corrigir G1
# UsuÃ¡rio: "VÃ¡ para a cozinha"
# G1: *comeÃ§a a andar*
# UsuÃ¡rio: "NÃ£o! Quarto!"
# G1: *para, replaneja*
# G1: "Ok, indo para o quarto"

# Implementar interrupÃ§Ã£o
@voice.on_interrupt()
def handle_interrupt(new_command):
    robot.stop()
    voice.speak("Desculpe, corrigindo...")
    execute_command(new_command)
```

</TabItem>
</Tabs>

### Feedback Durante ExecuÃ§Ã£o

```python
# G1 dÃ¡ updates verbais
@robot.on_task_progress()
def give_feedback(progress):
    milestones = {
        0.25: "Chegando na cozinha...",
        0.50: "Procurando o objeto...",
        0.75: "Pegando...",
        1.00: "Pronto!"
    }

    if progress in milestones:
        voice.speak(milestones[progress])
```

---

## ğŸšï¸ ConfiguraÃ§Ãµes AvanÃ§adas

### Volume e Velocidade de Fala

```python
# Ajustar TTS (Text-to-Speech)
voice.configure_tts(
    volume=0.7,              # 0-1
    speech_rate=1.0,         # 0.5-2.0 (1.0 = normal)
    pitch=1.0,               # 0.5-2.0
    voice_style="friendly"   # "professional", "friendly", "casual"
)

# Voz contextual
voice.set_contextual_tts({
    "urgent": {"rate": 1.3, "volume": 0.9},
    "quiet": {"rate": 0.8, "volume": 0.4},
    "default": {"rate": 1.0, "volume": 0.7}
})
```

### PersonalizaÃ§Ã£o de Voz

```python
# Voice cloning (MindOn Pro apenas)
voice.clone_voice(
    reference_audio="my_voice.wav",  # 30s de Ã¡udio
    voice_name="minha_voz"
)

# G1 agora fala com sua voz!
voice.set_tts_voice("minha_voz")
```

---

## âœ… Checklist de PrÃ¡tica

- [ ] Configurou wake word personalizada
- [ ] Testou 10 comandos de navegaÃ§Ã£o diferentes
- [ ] Executou pick-and-place via comando de voz
- [ ] Configurou idioma secundÃ¡rio (multilÃ­ngue)
- [ ] Testou contexto conversacional (3+ turnos)
- [ ] Configurou threshold de clarificaÃ§Ã£o
- [ ] Ajustou volume e velocidade de TTS
- [ ] Criou comando de voz customizado

---

## ğŸ› Troubleshooting Comum

| Problema | Causa | SoluÃ§Ã£o |
|----------|-------|---------|
| Wake word nÃ£o detecta | Sensibilidade baixa | Aumentar `sensitivity` para 0.8-0.9 |
| Muitos falsos positivos | Sensibilidade alta | Reduzir para 0.6-0.7, adicionar `noise_suppression` |
| NÃ£o entende comandos | RuÃ­do ambiente | Ativar `noise_suppression=True` |
| Sotaque nÃ£o reconhecido | Modelo nÃ£o treinado | Treinar custom wake word com 20+ amostras |
| LatÃªncia alta (>2s) | Usando cloud LLM | Mudar para on-device: `voice.set_llm_mode("edge")` |
| Responde em inglÃªs | Idioma errado | Verificar `voice.get_language()`, setar `pt-BR` |

---

## ğŸ”— PrÃ³ximos Passos

:::tip PrÃ³ximo MÃ³dulo
**[ğŸ“š Skills Library â†’](./skills-library)**

Explore a biblioteca de 500+ skills prÃ©-construÃ­das do MindOn.
:::

---

**â±ï¸ Tempo:** 45-60 min | **ğŸ® Hands-on:** Essencial | **ğŸ¤ Requisito:** Ambiente silencioso para testes
