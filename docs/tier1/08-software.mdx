---
id: software
title: ğŸ§  Software e InteligÃªncia Artificial
description: Os sistemas inteligentes que fazem humanoides pensar, ver e aprender
tier: 1
module_number: 8
estimated_time: 22
sidebar_position: 8
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# ğŸ§  Software e InteligÃªncia Artificial

<div className="hero-badges">
  <span className="badge badge-tier1">Tier 1 - MÃ³dulo 8</span>
  <span className="badge badge-time">â±ï¸ 22 minutos</span>
  <span className="badge badge-level">ğŸ“Š Fundamentos</span>
</div>

## ğŸ¯ O que vocÃª vai aprender

Neste mÃ³dulo, vocÃª vai descobrir o cÃ©rebro digital dos humanoides: ROS2, visÃ£o computacional, aprendizado por reforÃ§o, modelos de linguagem e como IA estÃ¡ revolucionando robÃ³tica.

---

## ğŸ—ï¸ Stack de Software

### Arquitetura em Camadas

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CAMADA 5: AplicaÃ§Ã£o                 â”‚
â”‚ - MissÃµes/Tarefas (pegar caixa)     â”‚
â”‚ - Interface com humano              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CAMADA 4: CogniÃ§Ã£o/IA               â”‚
â”‚ - GPT-4 (linguagem natural)         â”‚
â”‚ - YOLO (detecÃ§Ã£o de objetos)        â”‚
â”‚ - Planejamento de trajetÃ³ria        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CAMADA 3: Middleware (ROS2)         â”‚
â”‚ - Pub/Sub de dados                  â”‚
â”‚ - SLAM, navegaÃ§Ã£o                   â”‚
â”‚ - Controle de movimentos            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CAMADA 2: Drivers                   â”‚
â”‚ - Interface com sensores/motores    â”‚
â”‚ - RealSense, LIDAR, CANbus          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CAMADA 1: Sistema Operacional       â”‚
â”‚ - Ubuntu 22.04                      â”‚
â”‚ - Kernel Linux (drivers)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¤– ROS2 (Robot Operating System 2)

### O que Ã© ROS2?

**DefiniÃ§Ã£o:** Framework open-source para desenvolvimento de software de robÃ´s

**NÃƒO Ã© um sistema operacional**, Ã© uma camada de middleware sobre Linux.

<div className="feature-grid">

<div className="feature-card">
<div className="feature-icon">ğŸ“¡</div>
<h3>ComunicaÃ§Ã£o</h3>
<p>Publish/Subscribe entre processos</p>
</div>

<div className="feature-card">
<div className="feature-icon">ğŸ“¦</div>
<h3>Pacotes</h3>
<p>Milhares de bibliotecas prontas (SLAM, navegaÃ§Ã£o, etc)</p>
</div>

<div className="feature-card">
<div className="feature-icon">ğŸ› ï¸</div>
<h3>Ferramentas</h3>
<p>VisualizaÃ§Ã£o (RViz), simulaÃ§Ã£o (Gazebo), debug</p>
</div>

<div className="feature-card">
<div className="feature-icon">ğŸŒ</div>
<h3>Ecossistema</h3>
<p>Comunidade gigantesca, suporte industrial</p>
</div>

</div>

---

### Conceitos Fundamentais

<Tabs>
<TabItem value="nodes" label="Nodes (NÃ³s)" default>

### Nodes - Processos Independentes

**O que Ã©:** Programa que executa uma tarefa especÃ­fica

**Exemplo de arquitetura:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ camera_node  â”‚â”€â”€â”€â–¶â”‚ vision_node  â”‚â”€â”€â”€â–¶â”‚ planner_node â”‚
â”‚ (publica     â”‚    â”‚ (detecta     â”‚    â”‚ (decide onde â”‚
â”‚  imagens)    â”‚    â”‚  objetos)    â”‚    â”‚  ir)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                                        â”‚
        â–¼                                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  imu_node    â”‚                        â”‚ control_node â”‚
â”‚ (orientaÃ§Ã£o) â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ (move robÃ´)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Vantagens:**
- âœ… Cada node pode crashar sem derrubar sistema todo
- âœ… Desenvolvimento paralelo (times diferentes trabalham em nodes diferentes)
- âœ… ReutilizaÃ§Ã£o (node de cÃ¢mera funciona em qualquer robÃ´)

</TabItem>

<TabItem value="topics" label="Topics (TÃ³picos)">

### Topics - Canais de ComunicaÃ§Ã£o

**Publish/Subscribe Pattern:**
```python
# Node 1: Publica posiÃ§Ã£o da junta
import rclpy
from sensor_msgs.msg import JointState

publisher = node.create_publisher(JointState, '/joint_states', 10)

joint_msg = JointState()
joint_msg.name = ['knee_left', 'knee_right']
joint_msg.position = [1.57, 1.57]  # 90 graus em radianos

publisher.publish(joint_msg)


# Node 2: Subscreve posiÃ§Ã£o
def joint_callback(msg):
    print(f"Joelho esquerdo: {msg.position[0]} rad")

subscription = node.create_subscription(
    JointState,
    '/joint_states',
    joint_callback,
    10
)
```

**Topics tÃ­picos em humanoide:**
```
/camera/image_raw         (sensor_msgs/Image)
/imu/data                 (sensor_msgs/Imu)
/joint_states             (sensor_msgs/JointState)
/cmd_vel                  (geometry_msgs/Twist)
/tf                       (TransformaÃ§Ãµes 3D)
/odometry                 (nav_msgs/Odometry)
```

</TabItem>

<TabItem value="services" label="Services (ServiÃ§os)">

### Services - RequisiÃ§Ã£o/Resposta

**DiferenÃ§a de Topic:**
- **Topic:** Streaming contÃ­nuo (cÃ¢mera publicando imagens)
- **Service:** Chamada Ãºnica (ex: "calcule trajetÃ³ria para X")

**Exemplo:**
```python
# Servidor: Calcula cinemÃ¡tica inversa
from robot_interfaces.srv import InverseKinematics

def handle_ik_request(request, response):
    # request.target_position = [x, y, z]
    joint_angles = calculate_ik(request.target_position)
    response.joint_angles = joint_angles
    return response

service = node.create_service(
    InverseKinematics,
    'inverse_kinematics',
    handle_ik_request
)


# Cliente: Solicita cinemÃ¡tica
client = node.create_client(InverseKinematics, 'inverse_kinematics')
request = InverseKinematics.Request()
request.target_position = [0.5, 0.2, 0.8]  # x, y, z em metros

future = client.call_async(request)
# Quando retornar: future.result().joint_angles
```

</TabItem>

<TabItem value="actions" label="Actions (AÃ§Ãµes)">

### Actions - Tarefas Longas com Feedback

**Para quÃª:** OperaÃ§Ãµes que levam tempo e vocÃª quer progresso

**Exemplo - Caminhar atÃ© ponto:**
```python
# Action definition (walk_to_point.action)
# Goal
geometry_msgs/Point target
---
# Result
bool success
---
# Feedback
float32 distance_remaining
float32 percent_complete


# Cliente
action_client = ActionClient(node, WalkToPoint, 'walk_to_point')

goal = WalkToPoint.Goal()
goal.target = Point(x=2.0, y=1.0, z=0.0)

send_goal_future = action_client.send_goal_async(
    goal,
    feedback_callback=lambda fb: print(f"{fb.percent_complete}% done")
)

# Feedback aparece durante execuÃ§Ã£o:
# "25% done"
# "50% done"
# "75% done"
# "100% done"
```

</TabItem>
</Tabs>

---

### ROS2 vs ROS1

<div className="comparison-table">

| CaracterÃ­stica | ROS1 (2007-2020) | ROS2 (2017-hoje) |
|---------------|------------------|------------------|
| **ComunicaÃ§Ã£o** | Custom TCP | DDS (padrÃ£o industrial) |
| **Tempo-real** | âŒ NÃ£o | âœ… Sim (com RTOS) |
| **SeguranÃ§a** | âŒ Nenhuma | âœ… EncriptaÃ§Ã£o, autenticaÃ§Ã£o |
| **Multiplataforma** | SÃ³ Linux | Linux, Windows, macOS, RTOS |
| **Python** | 2.7 | 3.6+ |
| **Status** | Fim de vida 2025 | Ativo |

</div>

**Todos os humanoides novos usam ROS2** (Unitree, Figure, Apollo, etc.)

---

## ğŸ‘ï¸ VisÃ£o Computacional

### Pipeline de VisÃ£o

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CÃ¢mera   â”‚â”€â”€â–¶â”‚ PrÃ©-     â”‚â”€â”€â–¶â”‚ DetecÃ§Ã£o â”‚â”€â”€â–¶â”‚ DecisÃ£o  â”‚
â”‚ (1080p   â”‚   â”‚ processa-â”‚   â”‚ (IA)     â”‚   â”‚ (o que   â”‚
â”‚  30 FPS) â”‚   â”‚ mento    â”‚   â”‚          â”‚   â”‚  fazer)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚              â”‚
                    â–¼              â–¼
              [Resize,       [YOLO, SAM,
               Denoise,       Depth Est]
               CalibraÃ§Ã£o]
```

### Tarefas Comuns

<Tabs>
<TabItem value="detection" label="DetecÃ§Ã£o de Objetos" default>

### Object Detection - YOLO

**O que faz:** Identifica e localiza objetos em imagem

**Modelo popular: YOLOv8**
```python
from ultralytics import YOLO

model = YOLO('yolov8n.pt')  # Nano (rÃ¡pido)
results = model(image)

for detection in results[0].boxes:
    class_id = detection.cls
    confidence = detection.conf
    bbox = detection.xyxy  # [x1, y1, x2, y2]

    if class_id == 39:  # Classe "garrafa"
        print(f"Garrafa detectada em {bbox} com {confidence:.2f}")
```

**Performance no Jetson Orin:**
```
YOLOv8n (Nano): 120 FPS @ 640x480
YOLOv8s (Small): 80 FPS
YOLOv8m (Medium): 45 FPS
YOLOv8l (Large): 25 FPS
```

**Uso em humanoide:**
- "Pegue a garrafa azul na mesa" â†’ YOLO detecta garrafa

</TabItem>

<TabItem value="segmentation" label="SegmentaÃ§Ã£o">

### Semantic Segmentation

**O que faz:** Classifica cada pixel da imagem

**Exemplo - Segment Anything (Meta SAM):**
```python
from segment_anything import SamPredictor, sam_model_registry

sam = sam_model_registry["vit_b"](checkpoint="sam_vit_b.pth")
predictor = SamPredictor(sam)

predictor.set_image(image)

# Clicar em ponto para segmentar objeto
masks = predictor.predict(point_coords=[[320, 240]])
# masks[0] = pixels do objeto clicado
```

**Uso:**
- Identificar superfÃ­cies andar (chÃ£o vs parede vs escada)
- Segmentar objeto para pegar (qual parte Ã© a alÃ§a?)

</TabItem>

<TabItem value="depth" label="Estimativa de Profundidade">

### Depth Estimation

**OpÃ§Ãµes:**

**1. CÃ¢mera de profundidade (hardware):**
```python
import pyrealsense2 as rs

pipeline = rs.pipeline()
config = rs.config()
config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)

pipeline.start(config)
frames = pipeline.wait_for_frames()
depth_frame = frames.get_depth_frame()

distance = depth_frame.get_distance(320, 240)  # Centro da imagem
print(f"Objeto a {distance}m")
```

**2. IA (monocular depth):**
```python
from transformers import DPTForDepthEstimation

model = DPTForDepthEstimation.from_pretrained("Intel/dpt-large")
depth_map = model(image)

# depth_map: Matriz com profundidade estimada por pixel
```

**Trade-off:**
- Hardware (RealSense): Preciso, rÃ¡pido, mas sensor extra
- IA: SÃ³ cÃ¢mera RGB, menos preciso, mais processamento

</TabItem>

<TabItem value="pose" label="Pose Estimation">

### Human Pose Estimation

**O que faz:** Detecta esqueleto humano (17-21 keypoints)

**Modelo: OpenPose / MediaPipe:**
```python
import mediapipe as mp

mp_pose = mp.solutions.pose
pose = mp_pose.Pose()

results = pose.process(image)

if results.pose_landmarks:
    for landmark in results.pose_landmarks.landmark:
        # landmark.x, landmark.y, landmark.z
        print(f"Joelho: ({landmark.x}, {landmark.y})")
```

**Uso em humanoide:**
- Imitar movimentos humanos (motion capture)
- Detectar gestos ("acenar com mÃ£o" = comando)
- Evitar colisÃ£o com pessoa (saber onde estÃ£o membros)

</TabItem>
</Tabs>

---

## ğŸ§  InteligÃªncia Artificial

### Tipos de IA em Humanoides

<div className="feature-grid">

<div className="feature-card">
<div className="feature-icon">ğŸ¯</div>
<h3>Supervised Learning</h3>
<p>Treina com exemplos rotulados</p>
<p><strong>Uso:</strong> ClassificaÃ§Ã£o de objetos</p>
</div>

<div className="feature-card">
<div className="feature-icon">ğŸ®</div>
<h3>Reinforcement Learning</h3>
<p>Aprende por tentativa/erro</p>
<p><strong>Uso:</strong> LocomoÃ§Ã£o, manipulaÃ§Ã£o</p>
</div>

<div className="feature-card">
<div className="feature-icon">ğŸ¤</div>
<h3>Imitation Learning</h3>
<p>Copia demonstraÃ§Ãµes humanas</p>
<p><strong>Uso:</strong> Tarefas complexas</p>
</div>

<div className="feature-card">
<div className="feature-icon">ğŸ’¬</div>
<h3>Large Language Models</h3>
<p>Entende linguagem natural</p>
<p><strong>Uso:</strong> Interface com humano</p>
</div>

</div>

---

### Reinforcement Learning (RL)

<Tabs>
<TabItem value="concept" label="Conceito" default>

### Como Funciona

**Analogia:** Ensinar cachorro com petiscos

```
Estado (State):
  - PosiÃ§Ã£o de todas as juntas
  - Velocidades
  - OrientaÃ§Ã£o do tronco (IMU)
  - Alvo a alcanÃ§ar

AÃ§Ã£o (Action):
  - Torque aplicado em cada junta
  - Exemplo: [10, -5, 20, ...] NÂ·m

Recompensa (Reward):
  - +1: Se robÃ´ andou para frente
  - -10: Se robÃ´ caiu
  - +100: Se alcanÃ§ou objetivo

Objetivo:
  Maximizar recompensa acumulada
```

**Processo:**
```
1. RobÃ´ tenta andar (aÃ§Ã£o aleatÃ³ria)
2. Cai (recompensa -10)
3. IA ajusta: "Torque no tornozelo deve ser maior"
4. Tenta novamente...
5. ApÃ³s milhÃµes de tentativas: Aprende a andar
```

</TabItem>

<TabItem value="ppo" label="PPO (Algoritmo)">

### Proximal Policy Optimization

**O mais usado em robÃ³tica (2024)**

**Por quÃª:**
- âœ… EstÃ¡vel (nÃ£o diverge facilmente)
- âœ… Sample-efficient (aprende rÃ¡pido)
- âœ… Funciona bem em simulaÃ§Ã£o â†’ mundo real

**CÃ³digo simplificado:**
```python
import torch
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import SubprocVecEnv

# Criar mÃºltiplos ambientes em paralelo (simuladores)
env = SubprocVecEnv([lambda: HumanoidEnv() for _ in range(16)])

# Treinar
model = PPO(
    'MlpPolicy',
    env,
    learning_rate=3e-4,
    n_steps=2048,
    batch_size=64,
    n_epochs=10,
    verbose=1
)

model.learn(total_timesteps=10_000_000)  # 10M passos (~24h em GPU)
model.save("humanoid_walk")


# Usar modelo treinado
obs = env.reset()
for _ in range(1000):
    action, _ = model.predict(obs, deterministic=True)
    obs, reward, done, info = env.step(action)
```

**Sucesso real:**
- Boston Dynamics Atlas: Aprendeu parkour com RL
- Unitree H1: LocomoÃ§Ã£o dinÃ¢mica

</TabItem>

<TabItem value="sim2real" label="Sim-to-Real">

### TransferÃªncia SimulaÃ§Ã£o â†’ Real

**Problema:**
```
RL precisa de milhÃµes de tentativas
  â†“
Se treinar em robÃ´ real:
  - Levaria meses
  - RobÃ´ quebraria 1000x
  - Perigo para humanos
```

**SoluÃ§Ã£o: Treinar em simulador**

**Simuladores populares:**
- **Isaac Sim (NVIDIA):** GPU-accelerated, fotorrealista
- **MuJoCo (DeepMind):** FÃ­sica precisa, rÃ¡pido
- **Gazebo (Open Robotics):** Integrado com ROS2
- **PyBullet (Google):** Python-friendly

**Desafio: Reality Gap**
```
Simulador Ã© perfeito demais:
  - FÃ­sica ideal (sem atrito variÃ¡vel)
  - Sem ruÃ­do de sensores
  - Sem atrasos

RobÃ´ real:
  - Atrito muda com temperatura
  - Sensores ruidosos
  - LatÃªncia de comunicaÃ§Ã£o
```

**TÃ©cnica: Domain Randomization**
```python
# Randomizar parÃ¢metros na simulaÃ§Ã£o
for episode in training:
    # Aleatorizar fÃ­sica
    sim.set_friction(random.uniform(0.5, 1.5))
    sim.set_mass(random.uniform(45, 50))  # kg

    # Aleatorizar sensores
    imu_noise = random.normal(0, 0.1)
    encoder_delay = random.randint(1, 5)  # ms

    # Aleatorizar ambiente
    ground_texture = random.choice(['concrete', 'grass', 'carpet'])

# Resultado: PolÃ­tica robusta que funciona no mundo real
```

**Sucesso comprovado:**
- OpenAI: Cubo de Rubik resolvido por robÃ´ (treinado 100% em sim)
- Tesla: Optimus aprende tarefas em Isaac Sim

</TabItem>
</Tabs>

---

### Foundation Models (LLMs + VLMs)

<Tabs>
<TabItem value="llm" label="Language Models" default>

### GPT-4, Claude, etc. em Humanoides

**Como integram:**
```
Humano: "RobÃ´, pegue a garrafa azul que estÃ¡ na mesa e traga para mim"
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LLM (GPT-4)                â”‚
â”‚ Entende intenÃ§Ã£o:          â”‚
â”‚ 1. Localizar mesa          â”‚
â”‚ 2. Identificar garrafa azulâ”‚
â”‚ 3. Planejar caminho        â”‚
â”‚ 4. Executar pick & place   â”‚
â”‚ 5. Navegar atÃ© humano      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    ROS2 Actions
    (walk_to, pick_object, etc.)
```

**Exemplo real - Figure 02 + OpenAI:**
```python
# IntegraÃ§Ã£o via API
import openai

def process_command(speech_text, image):
    prompt = f"""
    VocÃª Ã© um robÃ´ humanoide. O humano disse: "{speech_text}"
    Imagem atual: [imagem da cÃ¢mera]

    Gere uma sequÃªncia de aÃ§Ãµes ROS2 para executar.
    """

    response = openai.ChatCompletion.create(
        model="gpt-4-vision",
        messages=[{
            "role": "user",
            "content": [
                {"type": "text", "text": prompt},
                {"type": "image_url", "image_url": image}
            ]
        }]
    )

    actions = parse_actions(response['choices'][0]['message']['content'])
    execute_actions(actions)
```

**Vantagens:**
- âœ… Interface natural (fala, nÃ£o cÃ³digo)
- âœ… RaciocÃ­nio complexo (LLM planeja passos)
- âœ… Contexto (lembra conversas anteriores)

**Desafios:**
- âŒ LatÃªncia (API call = 1-3s)
- âŒ AlucinaÃ§Ã£o (LLM pode gerar aÃ§Ã£o impossÃ­vel)
- âš ï¸ Requer validaÃ§Ã£o de seguranÃ§a

</TabItem>

<TabItem value="vlm" label="Vision-Language Models">

### VLMs (GPT-4V, Gemini, etc.)

**O que fazem:** Entendem imagem + texto juntos

**Exemplo de prompt:**
```
Imagem: [Foto da cozinha]
Pergunta: "Onde estÃ¡ a caneca?"

VLM: "A caneca estÃ¡ na mesa, Ã  esquerda do laptop,
      prÃ³ximo Ã  janela. Ã‰ uma caneca branca com
      alÃ§a voltada para a direita."
```

**Uso em robÃ³tica:**
```python
def find_object(object_name, camera_image):
    prompt = f"Localize {object_name} nesta imagem e me dÃª coordenadas (x, y) aproximadas."

    vlm_response = vlm.query(prompt, camera_image)
    # "O objeto estÃ¡ aproximadamente em (320, 240) pixels, centro-esquerda"

    # Converter pixel â†’ coordenadas 3D (com depth camera)
    x_pixel, y_pixel = extract_coordinates(vlm_response)
    depth = depth_camera.get_distance(x_pixel, y_pixel)

    world_coords = pixel_to_world(x_pixel, y_pixel, depth)
    return world_coords
```

**Modelos open-source:**
- **LLaVA:** 7B-13B params, roda em Jetson Orin
- **CLIP:** Associa imagem â†” texto
- **BLIP-2:** Image captioning

</TabItem>

<TabItem value="embodied" label="Embodied AI">

### IA Embodied (Incorporada)

**Conceito:** IA que aprende atravÃ©s de interaÃ§Ã£o fÃ­sica

**DiferenÃ§a:**
```
IA tradicional (GPT):
  Treina com texto da internet
  NÃ£o tem corpo
  NÃ£o entende fÃ­sica

IA Embodied:
  Treina controlando robÃ´
  Aprende "peso", "atrito", "equilÃ­brio"
  Conhecimento sensoriomotor
```

**Exemplo - RT-2 (Google):**
```
Modelo treinado com:
  - 13 robÃ´s diferentes
  - 100,000 horas de demonstraÃ§Ãµes
  - VisÃ£o + linguagem + aÃ§Ãµes

Capacidade:
  "Pegue a fruta com maior vitamina C"
  â†’ RobÃ´ identifica laranja (nÃ£o banana)
  â†’ Pega a laranja
  (Combinou conhecimento de LLM + visÃ£o + controle motor)
```

**TendÃªncia 2024-2025:**
- Modelos foundation para robÃ³tica (tipo "GPT para robÃ´s")
- Treinamento com dados de milhÃµes de robÃ´s
- Transfer learning entre diferentes humanoides

</TabItem>
</Tabs>

---

## ğŸ—ºï¸ SLAM e NavegaÃ§Ã£o

### SLAM (Simultaneous Localization and Mapping)

**Problema:** RobÃ´ em ambiente desconhecido precisa:
1. Saber onde estÃ¡ (localizaÃ§Ã£o)
2. Construir mapa (mapeamento)

**Paradoxo:** Para mapear, precisa saber posiÃ§Ã£o. Para saber posiÃ§Ã£o, precisa ter mapa!

**SoluÃ§Ã£o:** SLAM faz ambos simultaneamente

### Tipos de SLAM

<Tabs>
<TabItem value="lidar" label="LIDAR SLAM" default>

**Entrada:** Scans de LIDAR 2D/3D

**Algoritmos populares:**
- **Cartographer (Google):** Usado em robÃ´s do Alphabet
- **SLAM Toolbox:** ROS2 nativo
- **LIO-SAM:** LIDAR + IMU fusion

**Output:**
- Mapa 2D (occupancy grid)
- PosiÃ§Ã£o do robÃ´ (x, y, Î¸)

**PrecisÃ£o:** 1-5 cm

</TabItem>

<TabItem value="visual" label="Visual SLAM">

**Entrada:** CÃ¢meras RGB ou RGB-D

**Algoritmos:**
- **ORB-SLAM3:** Monocular/EstÃ©reo/RGB-D
- **RTAB-Map:** ROS2, RGB-D
- **OpenVSLAM:** Open-source

**Vantagem:**
- âœ… NÃ£o precisa de LIDAR (mais barato)
- âœ… InformaÃ§Ã£o semÃ¢ntica (cor, textura)

**Desvantagem:**
- âŒ Falha em ambientes sem features (parede branca)
- âŒ SensÃ­vel a iluminaÃ§Ã£o

</TabItem>
</Tabs>

---

## ğŸ› ï¸ Ferramentas de Desenvolvimento

### Simuladores

<div className="comparison-table">

| Simulador | FÃ­sica | GrÃ¡ficos | ROS2 | GPU | Ideal para |
|-----------|--------|----------|------|-----|------------|
| **Isaac Sim** | â­â­â­â­â­ | â­â­â­â­â­ | âœ… | NVIDIA | RL, visÃ£o |
| **Gazebo** | â­â­â­â­ | â­â­â­ | âœ… | Opcional | ROS dev |
| **MuJoCo** | â­â­â­â­â­ | â­â­ | âš ï¸ | Sim | RL research |
| **PyBullet** | â­â­â­ | â­â­ | âš ï¸ | Sim | Prototipagem |
| **Webots** | â­â­â­â­ | â­â­â­â­ | âœ… | NÃ£o | EducaÃ§Ã£o |

</div>

---

### VisualizaÃ§Ã£o - RViz2

**O que Ã©:** Ferramenta ROS2 para visualizar dados do robÃ´ em 3D

**Mostra:**
- Modelo 3D do robÃ´ (URDF)
- Nuvem de pontos (LIDAR)
- CÃ¢meras
- TrajetÃ³rias planejadas
- Mapa (SLAM)

**Screenshot tÃ­pico:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [Modelo 3D do H1]  [Nuvem LIDAR]   â”‚
â”‚                                     â”‚
â”‚ Juntas coloridas   ObstÃ¡culos em   â”‚
â”‚ por torque         vermelho         â”‚
â”‚                                     â”‚
â”‚ [Mapa 2D]          [TrajetÃ³ria]    â”‚
â”‚ Grid ocupado       Caminho planejadoâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’¡ TendÃªncias Futuras

<div className="feature-grid">

<div className="feature-card">
<div className="feature-icon">ğŸŒ</div>
<h3>Modelos Foundation para RobÃ³tica</h3>
<p>"ChatGPT para robÃ´s" - Um modelo que controla qualquer humanoide</p>
</div>

<div className="feature-card">
<div className="feature-icon">ğŸ”„</div>
<h3>Fleet Learning</h3>
<p>Milhares de robÃ´s compartilham aprendizado (como Tesla FSD)</p>
</div>

<div className="feature-card">
<div className="feature-icon">ğŸ§¬</div>
<h3>IA NeuromÃ³rfica</h3>
<p>Chips que imitam cÃ©rebro (1000x mais eficiente)</p>
</div>

<div className="feature-card">
<div className="feature-icon">ğŸ¤–</div>
<h3>End-to-End Learning</h3>
<p>CÃ¢mera â†’ IA â†’ Controle de motor (sem programaÃ§Ã£o manual)</p>
</div>

</div>

---

## ğŸ“ Conceitos-Chave

:::tip Lembre-se
1. **ROS2 Ã© padrÃ£o da indÃºstria** - Aprenda se quiser trabalhar com humanoides
2. **RL domina locomoÃ§Ã£o** - MilhÃµes de tentativas em simulador
3. **LLMs sÃ£o o futuro da interface** - "Fale com o robÃ´" ao invÃ©s de programar
4. **Sim-to-Real funciona** - Domain randomization Ã© chave
:::

:::info VocÃª Sabia?
O **Boston Dynamics Atlas aprendeu a fazer backflip com RL**! Mas levou 100 milhÃµes de iteraÃ§Ãµes em simulaÃ§Ã£o (equivalente a 278 anos de tentativas contÃ­nuas se fosse no robÃ´ real).
:::

---

## ğŸ”— PrÃ³ximo MÃ³dulo

Agora vamos ver onde tudo isso Ã© aplicado no mundo real:

<div className="next-module-cta">
  <a href="/HG1/docs/tier1/aplicacoes" className="cta-button">
    ğŸ­ MÃ³dulo 9: AplicaÃ§Ãµes PrÃ¡ticas â†’
  </a>
</div>

---

<div className="module-footer">
  <div className="footer-section">
    <h4>ğŸ¯ Teste Seus Conhecimentos</h4>
    <p>Por que treinar RL em simulaÃ§Ã£o ao invÃ©s do robÃ´ real?</p>
    <details>
      <summary>Ver Resposta</summary>
      <p><strong>3 razÃµes:</strong> 1) Velocidade - simulador roda 100-1000x mais rÃ¡pido que tempo real, 2) SeguranÃ§a - robÃ´ nÃ£o quebra/machuca ninguÃ©m, 3) Custo - nÃ£o precisa de hardware fÃ­sico.</p>
    </details>
  </div>

  <div className="footer-section">
    <h4>ğŸ“š Recursos</h4>
    <ul>
      <li><a href="https://docs.ros.org/en/humble/" target="_blank">ROS2 Documentation</a></li>
      <li><a href="https://docs.omniverse.nvidia.com/isaacsim/" target="_blank">NVIDIA Isaac Sim</a></li>
      <li><a href="https://stable-baselines3.readthedocs.io/" target="_blank">Stable Baselines3 (RL)</a></li>
    </ul>
  </div>
</div>
