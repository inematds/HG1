---
id: sensores
title: üéØ Sensores em Humanoides
description: Como rob√¥s humanoides percebem e entendem o mundo ao seu redor
tier: 1
module_number: 4
estimated_time: 18
sidebar_position: 4
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# üéØ Sensores em Humanoides

<div className="hero-badges">
  <span className="badge badge-tier1">Tier 1 - M√≥dulo 4</span>
  <span className="badge badge-time">‚è±Ô∏è 18 minutos</span>
  <span className="badge badge-level">üìä Fundamentos</span>
</div>

## üéØ O que voc√™ vai aprender

Neste m√≥dulo, voc√™ vai entender como rob√¥s humanoides "sentem" o mundo: desde sensores b√°sicos de posi√ß√£o at√© c√¢meras 3D e sistemas t√°teis, descobrindo o que cada tipo de sensor faz e por que s√£o essenciais.

---

## üß† Os Sentidos de um Humanoide

Assim como humanos t√™m 5 sentidos, humanoides precisam de sistemas sensoriais para:

<div className="feature-grid">

<div className="feature-card">
<div className="feature-icon">üëÅÔ∏è</div>
<h3>Vis√£o</h3>
<p>C√¢meras RGB, profundidade, LIDAR</p>
<p><strong>Fun√ß√£o:</strong> Ver obst√°culos, reconhecer objetos, navegar</p>
</div>

<div className="feature-card">
<div className="feature-icon">ü¶µ</div>
<h3>Propriocep√ß√£o</h3>
<p>Encoders, IMU, sensores de torque</p>
<p><strong>Fun√ß√£o:</strong> Saber posi√ß√£o de juntas e orienta√ß√£o do corpo</p>
</div>

<div className="feature-card">
<div className="feature-icon">ü§≤</div>
<h3>Tato</h3>
<p>Sensores de for√ßa, press√£o, t√°teis</p>
<p><strong>Fun√ß√£o:</strong> Detectar contato, for√ßa de aperto</p>
</div>

<div className="feature-card">
<div className="feature-icon">üéß</div>
<h3>Audi√ß√£o</h3>
<p>Microfones array</p>
<p><strong>Fun√ß√£o:</strong> Comandos de voz, localiza√ß√£o de som</p>
</div>

</div>

:::tip Analogia
Se o **c√©rebro** do rob√¥ √© o computador e os **m√∫sculos** s√£o os motores, os **sensores** s√£o os olhos, ouvidos e nervos que permitem perceber e reagir ao ambiente.
:::

---

## üì∑ Sensores de Vis√£o

### 1. C√¢meras RGB (Coloridas)

<div className="sensor-card">

**O que √©:** C√¢mera normal como a do seu celular

**Como funciona:**
- Captura imagens em cores (Red, Green, Blue)
- Resolu√ß√£o t√≠pica: 1080p (1920x1080) a 4K
- Taxa de quadros: 30-60 FPS

**Aplica√ß√µes em Humanoides:**
- ‚úÖ Reconhecimento de objetos (via deep learning)
- ‚úÖ Leitura de texto/c√≥digos QR
- ‚úÖ Detec√ß√£o de pessoas
- ‚ùå N√ÉO mede dist√¢ncia diretamente

**Modelos Comuns:**
- C√¢meras USB gen√©ricas
- Raspberry Pi Camera Module
- C√¢meras industriais (Basler, FLIR)

</div>

**Exemplo de uso:**
```python
# Detectar uma pessoa na imagem
import cv2

camera = cv2.VideoCapture(0)
detector = cv2.CascadeClassifier('haarcascade_frontalface.xml')

ret, frame = camera.read()
faces = detector.detectMultiScale(frame)
# faces cont√©m coordenadas (x, y, largura, altura) de cada rosto
```

---

### 2. C√¢meras de Profundidade (Depth Cameras)

<div className="sensor-card featured">

**O que √©:** C√¢mera que mede dist√¢ncia de cada pixel

**Tecnologias:**

#### **Est√©reo (2 c√¢meras)**
- Funciona como olhos humanos
- Calcula profundidade por triangula√ß√£o
- Exemplo: OAK-D, ZED 2

#### **Tempo de Voo (ToF)**
- Emite luz infravermelha e mede tempo de retorno
- Exemplo: Kinect Azure, Intel RealSense D455

#### **Luz Estruturada**
- Projeta padr√£o de luz e analisa distor√ß√£o
- Exemplo: Intel RealSense D435i (mais comum em humanoides)

</div>

**Especifica√ß√µes Intel RealSense D435i:**
```
Resolu√ß√£o profundidade: 1280x720
Alcance: 0.3m - 10m
FPS: At√© 90
Campo de vis√£o: 87¬∞ x 58¬∞
IMU integrada: Sim (aceler√¥metro + girosc√≥pio)
Pre√ßo: ~$300 USD
```

**Output:**
- **RGB**: Imagem colorida normal
- **Depth**: Mapa de profundidade (cada pixel = dist√¢ncia em metros)
- **Point Cloud**: Nuvem de pontos 3D (x, y, z)

**Por que √© essencial:**
- ‚úÖ Navegar sem bater em obst√°culos
- ‚úÖ Pegar objetos (saber dist√¢ncia)
- ‚úÖ Subir escadas (detectar degraus)

---

### 3. LIDAR (Light Detection and Ranging)

<div className="sensor-card">

**O que √©:** Laser que gira 360¬∞ medindo dist√¢ncias

**Como funciona:**
1. Emite pulsos de laser
2. Mede tempo que luz leva para voltar
3. Calcula dist√¢ncia: `d = (velocidade_da_luz √ó tempo) / 2`

**Tipos:**

| Tipo | Alcance | FPS | Pre√ßo | Uso em Humanoides |
|------|---------|-----|-------|-------------------|
| **2D** | 0.1-30m | 10-20 Hz | $100-500 | Navega√ß√£o plana |
| **3D** | 0.1-100m | 10-30 Hz | $1k-10k | Mapeamento 3D |
| **Estado S√≥lido** | 0.1-200m | 30+ Hz | $500-5k | Futuro (sem partes m√≥veis) |

**Modelos Populares:**
- **RPLidar A1** ($100): 2D, 12m, hobby
- **Ouster OS1** ($5k): 3D, 128 canais, profissional
- **Livox Mid-360** ($500): 3D, estado s√≥lido

</div>

**Vantagens vs C√¢mera de Profundidade:**
- ‚úÖ Alcance maior (at√© 100m vs 10m)
- ‚úÖ Funciona em qualquer ilumina√ß√£o
- ‚úÖ Precis√£o milim√©trica
- ‚ùå Mais caro
- ‚ùå Mais pesado/volumoso

**Unitree H1 usa:** LIDAR 3D (provavelmente Livox Mid-360)

---

## ü¶æ Sensores Proprioceptivos (Posi√ß√£o e Movimento)

### 4. Encoders (Sensores de Posi√ß√£o Angular)

<div className="sensor-card">

**O que √©:** Mede √¢ngulo de rota√ß√£o de cada junta

**Tipos:**

#### **Encoder Absoluto**
- Sabe posi√ß√£o exata mesmo ap√≥s desligar
- Mais caro
- Usado em juntas cr√≠ticas

#### **Encoder Incremental**
- Conta pulsos de rota√ß√£o
- Perde posi√ß√£o ao desligar (precisa "home position")
- Mais barato
- Mais comum

**Especifica√ß√µes T√≠picas:**
```
Resolu√ß√£o: 4096-65536 pulsos/revolu√ß√£o
Precis√£o: 0.01¬∞ - 0.001¬∞
Interface: SPI, I2C, RS-485
```

</div>

**Por que √© crucial:**
- Sem encoder, o rob√¥ **n√£o sabe onde suas pernas est√£o**
- Controle de posi√ß√£o depende 100% disso
- Exemplo: Para dobrar joelho a 90¬∞, precisa ler encoder

**Exemplo de leitura:**
```python
# Pseudoc√≥digo
encoder_value = read_encoder(joint_id=3)  # Joelho direito
angle_degrees = (encoder_value / 4096) * 360
print(f"Joelho est√° em {angle_degrees}¬∞")
```

---

### 5. IMU (Inertial Measurement Unit)

<div className="sensor-card featured">

**O que √©:** Chip que mede acelera√ß√£o e rota√ß√£o do corpo

**Componentes:**
- **Aceler√¥metro (3 eixos):** Mede acelera√ß√£o linear (m/s¬≤)
- **Girosc√≥pio (3 eixos):** Mede velocidade angular (¬∞/s)
- **Magnet√¥metro (3 eixos - opcional):** B√∫ssola digital

**IMU de 6 DoF vs 9 DoF:**
- **6 DoF**: Aceler√¥metro + Girosc√≥pio
- **9 DoF**: 6 DoF + Magnet√¥metro

</div>

**Aplica√ß√µes:**
- ‚úÖ Detectar se rob√¥ est√° caindo (inclina√ß√£o > 30¬∞)
- ‚úÖ Estimar orienta√ß√£o (roll, pitch, yaw)
- ‚úÖ Controle de equil√≠brio din√¢mico
- ‚úÖ Fus√£o sensorial (junto com encoders)

**Modelos Comuns:**
- **MPU-6050** ($2): Hobby, 6 DoF
- **BNO055** ($20): 9 DoF com fus√£o onboard
- **VectorNav VN-100** ($500): IMU industrial, precis√£o alta

**Dados fornecidos:**
```python
imu_data = {
    'accel': [0.05, -0.02, 9.81],  # m/s¬≤ (x, y, z)
    'gyro': [0.1, 0.0, -0.5],      # ¬∞/s (roll, pitch, yaw)
    'orientation': [0, 5, 180]     # Graus (calculado por fus√£o)
}
```

**Unitree H1:** IMU de 6 eixos no tronco + IMU na RealSense D435i

---

### 6. Sensores de For√ßa/Torque

<div className="sensor-card">

**O que √©:** Mede for√ßa aplicada em uma junta ou superf√≠cie

**Localiza√ß√µes T√≠picas:**

#### **1. P√©s (For√ßa de Contato)**
- 4 c√©lulas de carga por p√© (uma em cada canto)
- Mede for√ßa vertical (N) e centro de press√£o
- **Uso:** Detectar se p√© est√° no ch√£o, ZMP control

#### **2. Juntas (Torque)**
- Mede torque (N¬∑m) aplicado no motor
- **Uso:** Controle de imped√¢ncia, detec√ß√£o de colis√£o

#### **3. Punhos (Force/Torque Sensor)**
- Mede for√ßa em 3 eixos + torque em 3 eixos (6 DoF)
- **Uso:** Manipula√ß√£o delicada, montagem

</div>

**Exemplo - Sensor de P√©:**
```
Leitura:
  Frente-Esquerda: 15 N
  Frente-Direita:  18 N
  Tr√°s-Esquerda:   12 N
  Tr√°s-Direita:    14 N

Conclus√£o:
  For√ßa total: 59 N (~6 kg apoiado nesse p√©)
  Centro de press√£o: Levemente √† frente e direita
```

**Tesla Optimus:** Sensores de torque em **todas as 28 juntas**

---

## ü§≤ Sensores T√°teis (M√£os)

### 7. Sensores de Press√£o

<div className="sensor-card">

**O que √©:** Mede press√£o de contato na pele/superf√≠cie

**Tipos:**

#### **Resistivos (FSR - Force Sensitive Resistor)**
- Resist√™ncia muda com press√£o
- Barato ($1-5 por sensor)
- Precis√£o moderada

#### **Capacitivos**
- Capacit√¢ncia muda com proximidade/toque
- Detecta toque leve
- Usado em touchscreens

#### **Piezoresistivos**
- Alta precis√£o
- Caro ($50-500)
- Usado em grippers industriais

</div>

**Aplica√ß√µes:**
- ‚úÖ Detectar contato com objeto
- ‚úÖ Controlar for√ßa de aperto (n√£o esmagar ovo)
- ‚úÖ Feedback t√°til

**Exemplo - Gripper com FSR:**
```python
pressure = read_fsr(finger=1)  # 0-1023 (10 bits)

if pressure > 800:
    print("Aperto muito forte! Reduzir for√ßa")
    motor_force -= 10
elif pressure < 200:
    print("Objeto escorregando! Aumentar for√ßa")
    motor_force += 5
```

---

### 8. Pele Eletr√¥nica (E-Skin)

<div className="sensor-card">

**O que √©:** Matriz de sensores t√°teis distribu√≠dos

**Tecnologia:**
- Centenas de sensores em filme fino
- Cobre toda a m√£o/bra√ßo
- Mede press√£o, temperatura, proximidade

**Exemplo:** Sanctuary Phoenix Gen 7
- 20,000+ pontos de contato nas m√£os
- Resolu√ß√£o espacial: 1mm
- Feedback em tempo real

</div>

**Vantagens:**
- ‚úÖ Manipula√ß√£o de objetos fr√°geis (frutas, vidro)
- ‚úÖ Detec√ß√£o precoce de deslizamento
- ‚úÖ Intera√ß√£o segura com humanos

**Desafio:** Processamento de dados (20k sensores a 100 Hz = 2M leituras/segundo)

---

## üéß Sensores de √Åudio

### 9. Microfone Array

<div className="sensor-card">

**O que √©:** M√∫ltiplos microfones para localizar som

**Configura√ß√µes:**
- **2 microfones:** Est√©reo b√°sico
- **4 microfones:** Localiza√ß√£o 2D (√¢ngulo horizontal)
- **6+ microfones:** Localiza√ß√£o 3D

**Aplica√ß√µes:**
- ‚úÖ Reconhecimento de voz (Alexa, Siri)
- ‚úÖ Localiza√ß√£o de pessoa falando (girar cabe√ßa para quem fala)
- ‚úÖ Cancelamento de ru√≠do

</div>

**Exemplo - ReSpeaker Mic Array v2.0:**
```
- 4 microfones
- Alcance: 5 metros
- Algoritmo: Beamforming (foco direcional)
- Interface: USB
- Pre√ßo: $50
```

**Processamento:**
```python
# Detec√ß√£o de dire√ß√£o do som
angle = mic_array.get_sound_direction()
print(f"Som vindo de {angle}¬∞ (0¬∞ = frente)")

# Comando de voz
if "rob√¥, venha aqui" in speech_recognition():
    navigate_to(angle)
```

---

## üîÑ Fus√£o Sensorial

### Por que Combinar Sensores?

**Problema:** Nenhum sensor √© perfeito

| Sensor | Vantagem | Limita√ß√£o |
|--------|----------|-----------|
| **C√¢mera** | Rico em informa√ß√£o | Ruim em baixa luz |
| **LIDAR** | Preciso, qualquer luz | N√£o v√™ cores/texturas |
| **IMU** | R√°pido (1000 Hz) | Deriva ao longo do tempo |
| **Encoder** | Preciso | N√£o detecta deslizamento |

**Solu√ß√£o:** Fus√£o sensorial (combinar dados)

<Tabs>
<TabItem value="example1" label="Exemplo 1: Odometria" default>

### Estimativa de Posi√ß√£o

**Dados de entrada:**
- Encoders: "Perna moveu 30 cm"
- IMU: "Inclina√ß√£o de 5¬∞ para frente"
- C√¢mera: "Parede a 2 metros √† direita"

**Fus√£o (Extended Kalman Filter):**
```python
position_estimate = kalman_filter.update(
    encoder_data=leg_position,
    imu_data=orientation,
    vision_data=landmark_position
)

# Resultado: Posi√ß√£o (x, y, Œ∏) com incerteza reduzida
print(f"Rob√¥ est√° em ({position_estimate.x}, {position_estimate.y})")
```

**Precis√£o:**
- S√≥ encoder: ¬±10 cm (acumula erro)
- Encoder + IMU + C√¢mera: ¬±2 cm

</TabItem>

<TabItem value="example2" label="Exemplo 2: Equil√≠brio">

### Controle de Estabilidade

**Sensores:**
- IMU: Orienta√ß√£o do tronco
- Encoders: Posi√ß√£o de pernas
- Sensores de p√©: For√ßas de contato

**Controlador:**
```python
# A cada 1ms (1000 Hz)
def balance_controller():
    # Ler sensores
    tilt = imu.get_pitch()  # Inclina√ß√£o
    foot_forces = [left_foot.force, right_foot.force]

    # Detectar queda
    if abs(tilt) > 30:
        emergency_stop()

    # Ajustar torque das juntas
    for joint in leg_joints:
        torque = pid_controller.compute(
            target=0,  # Tronco vertical
            current=tilt,
            foot_contact=foot_forces
        )
        joint.set_torque(torque)
```

</TabItem>
</Tabs>

---

## üìä Stack Sensorial por Modelo

<div className="comparison-table">

| Sensor | Unitree H1 | Tesla Optimus | Figure 02 | Sanctuary Phoenix |
|--------|-----------|--------------|-----------|-------------------|
| **C√¢mera RGB** | 1x (D435i) | 8x (surround) | 2x (est√©reo) | 4x |
| **C√¢mera Depth** | 1x (D435i) | ‚ùå (s√≥ RGB) | 2x (est√©reo) | 2x ToF |
| **LIDAR** | ‚úÖ 3D | ‚ùå | ‚ùå | ‚ùå |
| **IMU** | 1x (6 DoF) | 1x (9 DoF) | 1x (6 DoF) | 2x (corpo+cabe√ßa) |
| **Encoders** | Todas juntas | Todas juntas | Todas juntas | Todas juntas |
| **For√ßa (P√©s)** | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| **Torque (Juntas)** | ‚ö†Ô∏è Principais | ‚úÖ Todas | ‚ö†Ô∏è Principais | ‚úÖ Todas |
| **T√°til (M√£os)** | ‚ùå | ‚ö†Ô∏è B√°sico | ‚ö†Ô∏è FSR | ‚úÖ‚úÖ E-Skin |
| **Microfone** | ‚ùå | ‚úÖ Array | ‚úÖ 2x | ‚úÖ Array |

</div>

**Tend√™ncia:** Menos sensores caros (LIDAR), mais c√¢meras + IA (Tesla approach)

---

## üõ†Ô∏è Construindo um Sistema Sensorial B√°sico

### Kit de Entrada ($500)

Para experimentar em projeto DIY:

```
1. C√¢mera RGB: Raspberry Pi Camera v2 ($25)
2. IMU: BNO055 ($20)
3. LIDAR 2D: RPLidar A1 ($100)
4. Encoders: AS5600 x 6 ($30)
5. FSR: 10 sensores ($20)
6. Microfone: ReSpeaker 2-Mic ($15)
7. Computador: Raspberry Pi 4 8GB ($75)
8. Cabos e estrutura ($100)

Total: ~$385 (resto para impress√£o 3D)
```

**O que voc√™ consegue fazer:**
- Navega√ß√£o b√°sica indoor
- Detec√ß√£o de obst√°culos
- Reconhecimento de voz
- Controle de juntas

---

## üí° Conceitos-Chave

:::tip Lembre-se
1. **Redund√¢ncia √© boa:** M√∫ltiplos sensores para mesma fun√ß√£o (backup)
2. **Taxa de atualiza√ß√£o importa:** IMU precisa ser r√°pido (1000 Hz), c√¢mera pode ser lenta (30 Hz)
3. **Fus√£o > Sensor isolado:** Combinar dados reduz incerteza
4. **Custo vs Benef√≠cio:** LIDAR √© √≥timo, mas c√¢mera depth √© 10x mais barata
:::

:::info Voc√™ Sabia?
O **Tesla Optimus usa apenas c√¢meras** (sem LIDAR) seguindo a filosofia dos carros Tesla. A ideia √© que IA pode aprender a estimar profundidade s√≥ com imagens, igual humanos fazem com 2 olhos.
:::

---

## üîó Pr√≥ximo M√≥dulo

Sensores **detectam** o mundo. Agora vamos aprender sobre **atuadores** - os motores que fazem o rob√¥ se mover:

<div className="next-module-cta">
  <a href="/HG1/docs/tier1/atuadores" className="cta-button">
    ü¶æ M√≥dulo 5: Atuadores e Controle ‚Üí
  </a>
</div>

---

<div className="module-footer">
  <div className="footer-section">
    <h4>üéØ Teste Seus Conhecimentos</h4>
    <p>Por que Unitree H1 usa LIDAR e Tesla Optimus n√£o?</p>
    <details>
      <summary>Ver Resposta</summary>
      <p><strong>Filosofias diferentes:</strong> Unitree prioriza precis√£o m√°xima (LIDAR), Tesla aposta em vis√£o computacional pura com IA (mais barato, escal√°vel). Ambas s√£o v√°lidas!</p>
    </details>
  </div>

  <div className="footer-section">
    <h4>üìö Recursos</h4>
    <ul>
      <li><a href="https://www.intelrealsense.com/depth-camera-d435i/" target="_blank">Intel RealSense D435i Specs</a></li>
      <li><a href="https://www.slamtec.com/en/Lidar/A1" target="_blank">RPLidar A1 Documentation</a></li>
      <li><a href="https://www.bosch-sensortec.com/products/smart-sensors/bno055/" target="_blank">BNO055 IMU Datasheet</a></li>
    </ul>
  </div>
</div>
